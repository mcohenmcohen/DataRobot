{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datarobot as dr\n",
    "import re\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# import seaborn as sns\n",
    "\n",
    "# specify your working derictory\n",
    "os.chdir(\"/Users/..\")\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "PROJECT_ID = 'project_id' #required\n",
    "MODEL_ID = 'model_id' #required\n",
    "\n",
    "USERNAME = 'your_username' #required\n",
    "API_TOKEN = 'your_api_token' #required\n",
    "\n",
    "PREDICTION_SERVER_URL = 'https://app.datarobot.com' #required\n",
    "PREDICTION_SERVER_KEY = '' #optional datarobot_key for dedicated servers, not needed for shared prediction server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify features for ploting PDP, target, input data file\n",
    "nSample = 1000\n",
    "feature1 = \"PrimaryDiagnosisType\" \n",
    "feature2 = 'DisablingEventType'\n",
    "target = \"claimduration\"\n",
    "inputFile = \"trainingdata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up datarobot\n",
    "dr.Client(endpoint='https://app.datarobot.com/api/v2', token=API_TOKEN)\n",
    "project = dr.Project.get(PROJECT_ID)\n",
    "model = dr.Model.get(project=PROJECT_ID,\n",
    "                     model_id=MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly reduces memory consumption, comment it out if you have in your dataset very large integer and/or float numbers\n",
    "def dataset_reduce_memory(data):\n",
    "    for c in data.select_dtypes(include=['float64']).columns:\n",
    "        data[c]=data[c].astype(np.float32)\n",
    "    for c in data.select_dtypes(include=['int64']).columns:\n",
    "        data[c]=data[c].astype(np.int32)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the training data\n",
    "rawData = pd.read_csv(inputFile, encoding='utf-8', engine='c')\n",
    "rawData = dataset_reduce_memory(rawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an artificial dataset\n",
    "np.random.seed(12345)\n",
    "small = rawData.sample(nSample).reset_index()\n",
    "# small.drop(['index'], axis=1, inplace=True)\n",
    "small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_class_types(data, feature1, feature2, n=50):\n",
    "    '''\n",
    "    A function to determine data types of chosen features and to generate n possible values of numeric feature(s) and/or up to 50 unique values of categorical feature(s)\n",
    "    Outliers of numeric features are pruned by filtering out 1st and 99th percentiles\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data : pd.DataFrame object\n",
    "        input dataset\n",
    "    feature1 : str\n",
    "        name of the feature1\n",
    "    feature2 : str\n",
    "        name of the feature2\n",
    "    n : int\n",
    "        number of values of numeric features to be drawn from np.linspace\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    v1 : np.array\n",
    "        generated values of first feature\n",
    "    v2 : np.array\n",
    "        generated values of second feature\n",
    "    dtype1 : str\n",
    "        datatype of the first feature\n",
    "    dtype2 : str\n",
    "        datatype of the second feature\n",
    "    '''\n",
    "    dtype1 = re.sub('\\d+','',str(rawData[feature1].dtype))\n",
    "    dtype2 = re.sub('\\d+','',str(rawData[feature2].dtype))\n",
    "    \n",
    "    # Change it if other data types are in your dataset\n",
    "    if(dtype1 in['int','float','double']): dtype1 = 'numeric' \n",
    "    if(dtype2 in['int','float','double']): dtype2 = 'numeric' \n",
    "    \n",
    "    print(dtype1, dtype2)\n",
    "    if dtype1 == 'object' and dtype2=='numeric':\n",
    "        v1 = list(set(data[feature1].values))\n",
    "        assert len(v1) <= 50 , \"Too many levels in categorical feature1\"\n",
    "        v2 = data[feature2].values\n",
    "        v2 = np.linspace(np.nanpercentile(v2, 1), np.nanpercentile(v2, 99), n, endpoint=True)\n",
    "        \n",
    "    elif dtype1 == 'numeric' and dtype2=='object':\n",
    "        v2 = list(set(data[feature2].values))\n",
    "        assert len(v2) <= 50 , \"Too many levels in categorical feature2\"\n",
    "        v1 = data[feature1].values\n",
    "        v1 = np.linspace(np.nanpercentile(v1, 1), np.nanpercentile(v1, 99), n, endpoint=True)\n",
    "        \n",
    "    elif dtype1 == 'numeric' and dtype2=='numeric':\n",
    "        v1 = data[feature1].values\n",
    "        v1 = np.linspace(np.nanpercentile(v1, 1), np.nanpercentile(v1, 99), n, endpoint=True)\n",
    "        v2 = data[feature2].values\n",
    "        v2 = np.linspace(np.nanpercentile(v2, 1), np.nanpercentile(v2, 99), n, endpoint=True)\n",
    "        assert len(v1) == len(v2), 'Smth is buggy'\n",
    "    \n",
    "    else: #'object' & 'object'\n",
    "        assert dtype1 == dtype2 == 'object' , \"Check data types\"\n",
    "        v1 = np.array(list(set(data[feature1].values)))\n",
    "        assert len(v1) <= 50 , \"Too many levels in categorical feature1\"\n",
    "        v2 = np.array(list(set(data[feature2].values)))\n",
    "        assert len(v2) <= 50 , \"Too many levels in categorical feature2\"\n",
    "        \n",
    "    return v1, v2, dtype1, dtype2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid of possible feature values\n",
    "v1, v2, dtype1, dtype2 = define_class_types(rawData, feature1, feature2)\n",
    "newValues = np.array(np.meshgrid(v1, v2)).reshape(2, len(v1)*len(v2)).T #equivalent to expand.grid in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for making predictions on new feature values\n",
    "artificial = small.loc[np.repeat(np.arange(0, small.shape[0]), len(newValues)),]\n",
    "artificial[feature1] = list(newValues[:,0]) * small.shape[0]\n",
    "artificial[feature2] = list(newValues[:,1]) * small.shape[0]\n",
    "artificial.drop(['index'], axis=1,inplace=True)\n",
    "artificial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version of prediction function\n",
    "# # NB! THIS TAKES TIME.\n",
    "# # function to upload dataset, predict and retrieve predictions from app.datarobot.com \n",
    "# def predict_artificial(artificial):\n",
    "#     dataset = project.upload_dataset(artificial)\n",
    "#     print('Dataset is uploaded')\n",
    "#     predict_job = model.request_predictions(dataset.id)\n",
    "#     predictions = predict_job.get_result_when_complete()\n",
    "#     print('Predictions are ready and retrieved') \n",
    "#     #TODO found out if time to predict N rows could be retrieved (as Model Info shows) to precalculate waiting time  \n",
    "#     assert artificial.shape[0] == predictions.shape[0], 'Something is wrong here'\n",
    "#     artificial[target] = predictions['prediction'].values\n",
    "#     # dataset.delete()#optional, cleaning\n",
    "#     return artificial\n",
    "\n",
    "# # artificial = predict_artificial(artificial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_scoring(dataset, shared_server=True, filepath_in='temp_scoring.csv', filepath_out='temp_preds.csv', n_samples=10000, n_concurrent=5):\n",
    "    '''\n",
    "    A function for prediction in batches using DataRobot API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    dataset : pd.DataFrame object\n",
    "        artificially created dataset for Partial Dependence Plot\n",
    "    shared_server : bool\n",
    "        Set to True when you want use a shared server, otherwise PREDICTION_SERVER_KEY must be specified and dedicated prediction servers are used\n",
    "    filepath_in : str\n",
    "        path to input file\n",
    "    filepath_out : str\n",
    "        path to temporary output file\n",
    "    n_samples : int\n",
    "        number of observation in dataset to predict in one batch\n",
    "    n_concurrent : int\n",
    "        number of concurrent prediction processes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataset : pd.DataFrame object\n",
    "        dataset with target column replaced by predictions\n",
    "    '''\n",
    "    #remove output file if already exists\n",
    "    if os.path.isfile(filepath_out): \n",
    "        os.remove(filepath_out)\n",
    "    \n",
    "    dataset.to_csv(filepath_in, index=False, encoding='utf-8')\n",
    "    print('Input dataset is stored in temp file\\nBatch scoring is started')\n",
    "    \n",
    "    # modify --api_version if you need\n",
    "    if shared_server:\n",
    "        cmd = (\"batch_scoring --host={0}/api --user=\\\"{1}\\\" --out={2} {3} {4} {5} --api_token={6} --datarobot_key={7} --api_version=predApi/v1.0 --timeout=600 --n_samples={8} --n_concurrent={9} --n_retry=5 --no\").format(\n",
    "            PREDICTION_SERVER_URL, USERNAME, filepath_out, PROJECT_ID, MODEL_ID, filepath_in, API_TOKEN, PREDICTION_SERVER_KEY, n_samples, n_concurrent)\n",
    "    else:\n",
    "        cmd = (\"batch_scoring --host={0}/api --user=\\\"{1}\\\" --out={2} {3} {4} {5} --datarobot_key={6} --timeout=600 --verbose --n_samples={7} --n_concurrent={8} --n_retry=5 --no\").format(\n",
    "            PREDICTION_SERVER_URL, USERNAME, filepath_out, PROJECT_ID, MODEL_ID, filepath_in, PREDICTION_SERVER_KEY, n_samples, n_concurrent)\n",
    "    # print(cmd)\n",
    "    os.system(cmd)\n",
    "    print('Batch scoring is finished')\n",
    "\n",
    "    preds = pd.read_csv(filepath_out, encoding='utf-8', engine='c')\n",
    "    preds = preds.sort_values('row_id')\n",
    "    assert dataset.shape[0] == preds.shape[0], 'Input and output dataset mismatch in rows number'\n",
    "    # assert dataset.index == preds.index, 'Input and output dataset mismatch in rows indexes'\n",
    "    dataset[target] = preds[target].values\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "artificial = predict_batch_scoring(artificial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute an average prediction per feature1 and feature2 value pair\n",
    "plot_data = artificial.groupby([feature1, feature2]).agg({target: \"mean\"}).reset_index()\n",
    "if dtype1 == 'numeric':\n",
    "    plot_data[feature1] = plot_data[feature1].astype(np.float32)\n",
    "\n",
    "# plot_data_pivot = plot_data.pivot(index=feature1, columns=feature2, values=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pd_num_obj(plot_data, feature1=feature1, feature2=feature2):\n",
    "    '''\n",
    "    plot a numeric-object or object-numeric line chart\n",
    "    '''\n",
    "    plt.style.use('seaborn-notebook')\n",
    "    palette = plt.get_cmap('Set1')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16,16))\n",
    "    num=0\n",
    "    for label, df in plot_data.groupby(feature2):\n",
    "        num+=1\n",
    "        plt.plot(df[feature1], df[target], marker='', color=palette(num), linewidth=1, alpha=0.9, label=label)\n",
    "     \n",
    "    ax.set_xticklabels(plot_data[feature1].values, rotation=45)\n",
    "    # Add legend\n",
    "    plt.legend(loc=2, ncol=1, title = feature2, fontsize = 10)#'best'\n",
    " \n",
    "    # Add titles\n",
    "    plt.title((\"Partial Dependence Plot Of {} \\nOn {} And {}\").format(target, feature1, feature2), loc='center', fontsize=16, fontweight=0, color='darkblue')\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(target)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #Some simple and usefull stuff:\n",
    "    #plot_data.set_index(feature1).sort_index().groupby(feature2)[target].plot(style='--o', legend=True)\n",
    "    #sns.pointplot(x=feature1, y=target, hue=feature2, data=plot_data)\n",
    "    \n",
    "    # print('Convenience plot with ``partial_dependence_plots``')\n",
    "    # XX, YY = np.meshgrid(np.array(plot_data_pivot.index), np.array(plot_data_pivot.columns))\n",
    "    # Z = np.array(plot_data_pivot.T)\n",
    "    # plt.contour(XX, YY, Z, colors='black');\n",
    "\n",
    "# plot_pd_num_obj(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pd_obj(plot_data):\n",
    "    '''\n",
    "    plot a heatmap of object features\n",
    "    '''\n",
    "    plot_data = plot_data.pivot(index=feature1, columns=feature2, values=target)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig, ax = plt.subplots(1,1, figsize=(16,16))\n",
    "    heatmap = ax.imshow(plot_data, cmap='BuPu')\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(plot_data.columns)))\n",
    "    ax.set_yticks(np.arange(len(plot_data.index)))    \n",
    "    \n",
    "    ax.set_xticklabels(plot_data.columns, rotation=45)\n",
    "    ax.set_yticklabels(plot_data.index)\n",
    "    \n",
    "    ax.set_title((\"Partial Dependence Of {} \\nOn Categorical Variables {} And {}\").format(target, feature1, feature2))\n",
    "    ax.set_xlabel(feature1)\n",
    "    ax.set_ylabel(feature2)\n",
    "    \n",
    "    plt.colorbar(heatmap)\n",
    "    plt.show()\n",
    "    \n",
    "# plot_pd_obj(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a 3d scatterplot\n",
    "def plot_pd_numerics(plot_data):\n",
    "    '''\n",
    "    plot a 3d scatterplot of numeric features\n",
    "    '''\n",
    "    plot_data_pivot = plot_data.pivot(index=feature1, columns=feature2, values=target)\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "\n",
    "    XX, YY = np.meshgrid(np.array(plot_data_pivot.index), np.array(plot_data_pivot.columns))\n",
    "    Z = np.array(plot_data_pivot)\n",
    "    ax = Axes3D(fig)\n",
    "    surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1,\n",
    "                       cmap=plt.cm.BuPu, edgecolor='k')\n",
    "    ax.set_xlabel(feature1)\n",
    "    ax.set_ylabel(feature2)\n",
    "    ax.set_zlabel('Partial dependence')\n",
    "    #  pretty init view\n",
    "    ax.view_init(elev=22, azim=122)\n",
    "    plt.colorbar(surf)\n",
    "    plt.suptitle('Partial Dependence Plot Of {} \\nOn average {} And {}'.format(target, feature1, feature2))\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot_pd_numerics(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_partial_dependence(plot_data, dtype1, dtype2):\n",
    "    '''\n",
    "    main plotting function\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    plot_data : pd.DataFrame object\n",
    "        dataset for Partial Dependence Plot with target column\n",
    "    dtype1 : str\n",
    "        datatype of the first feature\n",
    "    dtype2 : str\n",
    "        datatype of the second feature\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Partial dependence plot shown inline\n",
    "    '''\n",
    "    if dtype1 == 'numeric' and dtype2 == 'object':\n",
    "        plot_pd_num_obj(plot_data)\n",
    "    \n",
    "    if dtype1 == 'object' and dtype2 == 'numeric':\n",
    "        f1, f2 = feature2, feature1\n",
    "        plot_pd_num_obj(plot_data, f1, f2)\n",
    "    \n",
    "    if dtype1 == 'numeric' and dtype2 == 'numeric':\n",
    "        plot_pd_numerics(plot_data)\n",
    "    \n",
    "    if dtype1 == 'object' and dtype2 == 'object':\n",
    "        plot_pd_obj(plot_data)\n",
    "\n",
    "plot_partial_dependence(plot_data, dtype1, dtype2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
