{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Max `number_of_new_images` for Image Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-fa430eea03b5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0;31m#text_captions = pd.DataFrame(test_data)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mutah_images\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mutah\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'sq_ft'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'description'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'garage_type'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'amenities'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'heating'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"~/workspace/data-science-scripts/jjohnson/text_cols.csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-fa430eea03b5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0;31m#text_captions = pd.DataFrame(test_data)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mutah_images\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mutah\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'sq_ft'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'description'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'garage_type'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'amenities'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'heating'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"~/workspace/data-science-scripts/jjohnson/text_cols.csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_38_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_38_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_38_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_38_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_38_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1147\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1162\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "utah = pd.read_csv(\"~/Downloads/UtahHouseScore1.csv\")\n",
    "utah_images = pd.read_csv(\"~/Downloads/utah_house_score1.csv\")\n",
    "utah.head()\n",
    "list(utah.columns)\n",
    "numeric = pd.concat([utah_images.iloc[:99], utah[['sq_ft', 'bedrooms', 'bathrooms', 'patios', 'sq_ft']]], axis=1)\n",
    "numeric.head()\n",
    "numeric.to_csv('~/workspace/data-science-scripts/jjohnson/num_cols.csv')\n",
    "\n",
    "category = pd.concat([utah_images.iloc[:99], utah[['sq_ft', 'school_district', 'elementary', 'jr_high', 'high_school']]], axis=1)\n",
    "category.to_csv('~/workspace/data-science-scripts/jjohnson/category_cols.csv')\n",
    "\n",
    "captions = pd.read_csv('~/Downloads/captions.txt')\n",
    "\n",
    "test_data = []\n",
    "cols = 250\n",
    "rows = 305\n",
    "test_df = pd.DataFrame()\n",
    "df = pd.DataFrame(np.zeros([rows, cols])*np.nan)\n",
    "\n",
    "for i in range(rows):\n",
    "    samples = captions['caption'].sample(cols)\n",
    "    samples.reset_index(drop=True)\n",
    "    df.iloc[i] = samples\n",
    "\n",
    "bigtext = pd.concat([utah_images, df], axis=1)\n",
    "bigtext.to_csv(\"~/workspace/data-science-scripts/jjohnson/text_250cols.csv\")\n",
    "#text_captions = pd.DataFrame(test_data)\n",
    "\n",
    "text = pd.concat([utah_images, utah[['sq_ft', 'description', 'garage_type', 'amenities', 'heating']]], axis=1)\n",
    "text.to_csv(\"~/workspace/data-science-scripts/jjohnson/text_cols.csv\")\n",
    "\n",
    "geo = pd.concat([utah_images.iloc[:99], utah[['sq_ft', 'zip_geometry', 'latitude', 'longitude']]], axis=1)\n",
    "geo['zip_geo2'] = geo['zip_geometry']\n",
    "geo.to_csv(\"~/workspace/data-science-scripts/jjohnson/geo_cols.csv\")\n",
    "\n",
    "images = utah_images.copy()\n",
    "geo = pd.concat([utah_images.iloc[:99], utah[['sq_ft']]], axis=1)\n",
    "images['img2'] = images['image']\n",
    "images['img3'] = images['image']\n",
    "images['img4'] = images['image']\n",
    "images['img5'] = images['image']\n",
    "images.to_csv('~/workspace/data-science-scripts/jjohnson/image_cols.csv')\n",
    "\n",
    "mixed = pd.concat([utah_images.iloc[:99], utah.copy()], axis=1)\n",
    "mixed.to_csv(\"~/workspace/data-science-scripts/jjohnson/mixed.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#utah = pd.read_csv(\"~/Downloads/Utah-250.csv\")\n",
    "\n",
    "\n",
    "# Data is leaderboard exports of Imange Tunning Yaml run through shrink.\n",
    "hundred = pd.read_csv(\"./100x.csv\")\n",
    "fivehundred = pd.read_csv(\"500x.csv\")\n",
    "twentyfive = pd.read_csv(\"25x.csv\")\n",
    "ten = pd.read_csv(\"10x.csv\")\n",
    "five = pd.read_csv(\"5x.csv\")\n",
    "mixed_750 = pd.read_csv(\"~/Downloads/img_aug_mixed_ds_x750_lb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 10000)\n",
    "\n",
    "# some data doesn't have max ram gb calculated\n",
    "hundred['Max RAM GB'] = hundred['Max_RAM'] / 1000000000\n",
    "ten['Max RAM GB'] = ten['Max_RAM'] / 1000000000\n",
    "five['Max RAM GB'] = five['Max_RAM'] / 1000000000\n",
    "mixed_750['Max RAM GB'] = mixed_750['Max_RAM'] / 1000000000\n",
    "\n",
    "\n",
    "# combine them all and use total images moving forward\n",
    "all_data = pd.concat([five, ten, twentyfive, hundred, fivehundred, mixed_750], axis=0)\n",
    "all_data['total images'].update(all_data.pop('max total images'))\n",
    "\n",
    "# the assumption is that final task and network type will have a large effect\n",
    "# so pul those out for graphing purpose\n",
    "main_tasks =all_data.main_task.unique()\n",
    "main_tasks_dict = dict( zip(main_tasks, range(1, len(main_tasks) + 1)))\n",
    "all_data['main_task_int'] = all_data.main_task.apply(lambda x: main_tasks_dict[x])\n",
    "\n",
    "all_data['network_type'] = all_data.Blueprint.str.extract('net=([^;]*)', expand=False)\n",
    "all_data['network_type'] = all_data['network_type'].fillna('None')\n",
    "net_types = all_data['network_type'].unique()\n",
    "net_types_dict = dict( zip(net_types, range(1, len(net_types) +1)))\n",
    "all_data['network_type_int'] = all_data.network_type.apply(lambda x: net_types_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# leaderboard export of failed runs won't have MAX RAM GB\n",
    "valid_ram_data = all_data[all_data['Max RAM GB'] > 0]\n",
    "valid_ram_data = valid_ram_data.sort_values(by=['Max RAM GB'], ascending=False)\n",
    "\n",
    "# filter total amount of data, for better visibility (currently to keep graphs as same scale)\n",
    "#valid_ram_data = valid_ram_data[valid_ram_data['total images'] < 1000000]\n",
    "\n",
    "# mobilenet shows two distintive trend lines for memory use (graphs below)\n",
    "# to determine what was different between the two trends I split them so I could let DR\n",
    "# train for `high_mem_usage` and tell me why the difference\n",
    "valid_ram_data['images_per_GB'] = valid_ram_data['total images'] / valid_ram_data['Max RAM GB']\n",
    "valid_ram_data['high_mem_usage'] = valid_ram_data['images_per_GB'] < 11000\n",
    "valid_ram_data.index = range(len(valid_ram_data))\n",
    "\n",
    "\n",
    "# I want to use word cloud for the blueprint field, so this string manipulations were done\n",
    "# iteratively to filter out noise in the word cloud\n",
    "# remove uuid\n",
    "valid_ram_data[\"Blueprint\"] = valid_ram_data[\"Blueprint\"].str.replace(r'[0-9a-f]{24}','', regex=True)\n",
    "# SCPICK ids\n",
    "valid_ram_data[\"Blueprint\"] = valid_ram_data[\"Blueprint\"].str.replace(r\"cn=[0-9a-f]+\\'\",'', regex=True)\n",
    "# try to seperate paramets into things DR will tokenize\n",
    "valid_ram_data[\"Blueprint\"] = valid_ram_data[\"Blueprint\"].str.replace(';',' ')\n",
    "valid_ram_data[\"Blueprint\"] = valid_ram_data[\"Blueprint\"].str.replace('.', '_')\n",
    "valid_ram_data[\"Blueprint\"] = valid_ram_data[\"Blueprint\"].str.replace('=','oo')\n",
    "# remove dense array data for LR1 as it signals LR1 anyways, and this will cleanup the word cloud\n",
    "valid_ram_data[\"Blueprint\"] = valid_ram_data[\"Blueprint\"].str.replace(r'Coo\\[.*\\]', 'dense_array_data', regex=True)\n",
    "\n",
    "\n",
    "# break up the data into networks\n",
    "squeezenet = 'netoosqueezenet'\n",
    "efficientnet = 'netooefficientnet'\n",
    "mobilenet = 'netoomobilenet'\n",
    "\n",
    "mobilenet_data = valid_ram_data[valid_ram_data['Blueprint'].str.contains(mobilenet)]\n",
    "efficientnet_data = valid_ram_data[valid_ram_data['Blueprint'].str.contains(efficientnet)]\n",
    "squeezenet_data = valid_ram_data[valid_ram_data['Blueprint'].str.contains(squeezenet)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to use in DR\n",
    "valid_ram_data.to_csv(\"valid_ram_data.csv\")\n",
    "no_squeeze = valid_ram_data[~valid_ram_data['Blueprint'].str.contains(squeezenet)]\n",
    "no_squeeze.to_csv(\"no_squeeze_blueprint_not_id_valid_ram_data.csv\")\n",
    "mobile_net_only = no_squeeze[~no_squeeze['Blueprint'].str.contains(efficientnet)]\n",
    "mobile_net_only.to_csv(\"mobilenet_valid_ram_data.csv\")\n",
    "#all_data.to_csv(\"all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# graph by network, and split tasks by main_task inorder to find what network / task is the most greedy memory wise\n",
    "# if we can prevent that combination from OOMing then we will prevent all combinations, this allows us to keep\n",
    "# the metric (number_of_new_images) project based, otherwise I feel like we would need to do it for each (group) model\n",
    "\n",
    "# The graphs seemed to show that mobilenet with SGDC is the algorithm to hit 60GB of memory with the least amount of\n",
    "# images\n",
    "# graph is Max Ram vs Main Task code, size of dots is the number of images in the dataset.\n",
    "\n",
    "def plot_data(valid_ram_data, axis, title, color='red'):\n",
    "    scatter = axis.scatter(valid_ram_data['main_task'], #valid_ram_data['size_factor'], \n",
    "            valid_ram_data['Max RAM GB'], \n",
    "            c=valid_ram_data['dataset_x_cols'],\n",
    "            s=valid_ram_data['total images'] / 100,\n",
    "            alpha=0.5)\n",
    "    axis.set_title(title)\n",
    "    # produce a legend with the unique colors from the scatter\n",
    "    legend1 = axis.legend(*scatter.legend_elements(),\n",
    "                        loc=\"lower left\", title=\"# cols\")\n",
    "    axis.add_artist(legend1)\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "fig.suptitle(\"Ram usage per network\")\n",
    "plot_data(squeezenet_data, axs[0], \"squeezenet\")\n",
    "plot_data(efficientnet_data, axs[1], \"efficientnet\")\n",
    "plot_data(mobilenet_data, axs[2], \"mobilenet\")\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(21,18, forward=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GB vs total images x final modeler x network\n",
    "from matplotlib import ticker\n",
    "\n",
    "@ticker.FuncFormatter\n",
    "def get_task_name(task_int, pos):\n",
    "    for key, val in main_tasks_dict.items():\n",
    "        if val == task_int:\n",
    "            return key\n",
    "    return task_int\n",
    "\n",
    "def plot_data(valid_ram_data, axis, title, color='red'):\n",
    "    scatter = axis.scatter(valid_ram_data['total images'],\n",
    "            valid_ram_data['Max RAM GB'], \n",
    "            #s=valid_ram_data['dataset_x_image'],\n",
    "            c=valid_ram_data['main_task_int'],\n",
    "            label=valid_ram_data['main_task'],\n",
    "            alpha=0.5)\n",
    "    axis.set_title(title)\n",
    "    \n",
    "    legend1 = axis.legend(*scatter.legend_elements(prop='colors', fmt=get_task_name),\n",
    "                    loc=\"best\", title=\"Task Codes\")\n",
    "    axis.add_artist(legend1)\n",
    "\n",
    "    axis.grid(True)\n",
    "   \n",
    "\n",
    "mobilenet_data.index = range(len(mobilenet_data))\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "plot_data(mobilenet_data, axs[0], \"Mobilenet Pruned mem usage per image\")\n",
    "plot_data(squeezenet_data, axs[1], \"Squeezenet mem usage per image\")\n",
    "plot_data(efficientnet_data, axs[2], \"EfficientNet Pruned mem usage per image\")\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18,6, forward=True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Graph Max RAM GB vs number of rows, per network, split tick data into high_mem_usage (shown in yellow)\n",
    "# and not high_mem_usage shown in purple\n",
    "\n",
    "def plot_data(valid_ram_data, axis, title, color='red'):\n",
    "    axis.scatter(valid_ram_data['total images'],\n",
    "            valid_ram_data['Max RAM GB'], \n",
    "            c=valid_ram_data['high_mem_usage'],\n",
    "            #s=valid_ram_data['dataset_x_cols'] * 50,\n",
    "            alpha=0.5)\n",
    "    axis.set_title(title)\n",
    "   \n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "plot_data(mobilenet_data, axs[0], \"Mobilenet high vs norm mem usage per image\")\n",
    "# high_mem_mobilenet = mobilenet_data[mobilenet_data['high_mem_usage'] == True]\n",
    "# axs[0].plot(high_mem_mobilenet['total images'], high_mem_mobilenet['Max RAM GB'])\n",
    "# draw a line between max and min points\n",
    "chosen_line = {'x': [7295.0, 281500.0], 'y': [1.333252, 59.120000]}\n",
    "axs[0].plot(chosen_line['x'], chosen_line['y'])\n",
    "# so limit equation is y=0.00021074286756259003x−0.20411721886909362\n",
    "plot_data(squeezenet_data, axs[1], \"Squeezenet high vs norm mem usage per image\")\n",
    "plot_data(efficientnet_data, axs[2], \"Efficientnet high vs norm mem usage per image\")\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18,6, forward=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_data[mobilenet_data['high_mem_usage'] == True][['total images', 'Max RAM GB', 'dataset_x_cols']].sort_values(by=\"Max RAM GB\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so to explain what it takes to hit the yellow high mem target for Mobilenet, I ran several text models predicting\n",
    "# `high_mem_usage` and looked at the word cloud...\n",
    "# I manipulated the blueprint manually to get the word cloud to focus on different pices... removing information\n",
    "# about the exact dense array used by `LR1` ... i.e. converting C=[<some dense array>] to `C=dense_array_data`\n",
    "# then the word cloud looks like this:\n",
    "# dense_array_data, lr1 and poo1 (originally p==1) are all proxy features for using the LR1 model, so that's the memory\n",
    "# hog\n",
    "from IPython.display import Image\n",
    "Image(filename=\"LR1_word_cloud.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zooming out a bit by including the dense_array_data in the blueprint field then ngrams with highes coef include\n",
    "# (very small and hard to see):\n",
    "# batch_size_per_tow ... which is a proxy for KERASMULTIC\n",
    "# n=1000 ... number of trees I think so signifying a tree based model..\n",
    "# and to a lesser degree, parameters about where to \n",
    "\n",
    "# the wordcloud data shows (similar as above graph(s)) that for mobilenet its the final modeler task code \n",
    "# in addition to total_number_images that predicts the memory used.\n",
    "#\n",
    "# the task codes that correspond to the much steeper memory use curve are:\n",
    "# KERASMULTIC, LR1, ESLGBMTC, and SGDC\n",
    "Image(filename=\"mobilenet_wordcloud.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# various phrases that word cloud showed having high positive coef\n",
    "# `pp_sf` = stack fit it has coef of 1.0 for high_mem_usage... for image aug task we generally set it to 5\n",
    "# `batch_size_per_row` = values seem to have a high coef with heavy memory use used in KERASMULTIC\n",
    "# `t_n=5` == 5 CV folds which indicates more memory usage \n",
    "# n=1000   == number of trees I belive, used only in the two tree based models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}