{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Internal DataRobot or GoPuff Only*\n",
    "# *!!! Do not share with customers !!!*\n",
    "# Choosing the Optimum Selections per Customer\n",
    "#### Overview\n",
    "In this notebook, we continue our exploration from a marketing use-case that predicts the likelihood to respond to several different methods of outreach.\n",
    "For each customer, we have several different \"methods\" of reaching out to them:\n",
    "- `email`\n",
    "- `push` (an in-app push notificaton to the device)\n",
    "- `webhook` (a text message to the customer's cell phone number)\n",
    "\n",
    "We can also consider several \"journeys\" that hit several channels within the same hour or so:\n",
    "\n",
    "- `email + push`\n",
    "- `email + push + push`\n",
    "- `email + push + push + push`\n",
    "- `email + webhook`\n",
    "- `push + push`\n",
    "- `push + push + push`\n",
    "\n",
    "In addition to those 9 different choices, we can choose when to initiate the outreach:\n",
    "- `day of week` (7 different choices)\n",
    "- `hour of day` (24 different choices)\n",
    "\n",
    "#### Background\n",
    "In previous work, we've already designed the dataset and built a model to predict the `probability of response` within 12 hours. It is important to note that this was only possible because the customer had enough history where they had done outreach on all days of the week as well as many different hours of the day.\n",
    "\n",
    "\n",
    "\n",
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory is:  /Users/josh.berry/_use cases/gopuff/outbound channel optimization/best action with optimization app/notebooks\n"
     ]
    }
   ],
   "source": [
    "### Standard Imports - Sorry PEP8 fans, do not look below\n",
    "import pandas as pd, numpy as np, os, re, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "## Specific Imports\n",
    "\n",
    "#### I save my API token as an environmental variable.\n",
    "if os.environ.get(\"DR_API_TOKEN\") == None:\n",
    "    API_TOKEN = \"OR__pasteyourtokenherefromthedatarobotbyclickinginthetopright\"\n",
    "else:\n",
    "    API_TOKEN = os.environ.get(\"DR_API_TOKEN\")\n",
    "ENDPOINT_URL = \"https://app.datarobot.com/api/v2\"\n",
    "\n",
    "# Optimizer app credentials\n",
    "app_url = 'https://5ebe59b00c7ebb310dae5fc1.apps.datarobot.com'\n",
    "token = '-e0wiZY9tIORyojg2vkGDDMdt9LqcwZYrfwOq3qC4ag'\n",
    "\n",
    "# Deployment id\n",
    "deployment_id = '5ec4671924f0ff0797025582'\n",
    "\n",
    "### Display options for notebooks\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 25)\n",
    "\n",
    "### set path directories\n",
    "curr_dir = Path(os.getcwd())\n",
    "print('Current Directory is: ', str(curr_dir))\n",
    "data_dir = Path(curr_dir.parents[0] / 'data/')\n",
    "artifacts_dir = Path(curr_dir.parents[0] / 'artifacts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Common project specific variables\n",
    "FILENAME = '1K_to_score_example.csv'  # original data\n",
    "\n",
    "FILENAME_OPT_CACHE = 'optimizer_cache.csv'\n",
    "FILENAME_OPT_RESULT = 'optimizer_answer.csv'\n",
    "\n",
    "FILENAME_BRUTE_CACHE = 'temp2.csv'\n",
    "FILENAME_BRUTE_RESULT = 'temp2_scored.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to reduce memory footprint of the dataframe\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    import numpy as np\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} MB ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.10 MB (46.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "indata = reduce_mem_usage(pd.read_csv(Path(data_dir) / FILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "We'll use a random sample of our training data to illustrate how we would take new data, and figure out the best channel to use for each customer, to maximize the probability of response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 23,  6,  0, 13, 20,  1, 22, 10,  4, 17, 21, 19,  2,  9,  3,  8,\n",
       "       16,  5, 14, 15, 12,  7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indata['event_time_hour_of_day'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) increase dataframe size for more testing\n",
    "times_to_repeat = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll delete the target variable, since we wouldn't know it, and we'll blank out the columns which comes from \"the future\" of our invisible snapshot date which was used to query this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = indata.copy()   \n",
    "df = pd.concat([df]*times_to_repeat, ignore_index=True)\n",
    "df['fake_customerid'] = df.index\n",
    "df.drop(columns=['has_session_w_atc','z_cost'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign_concat_str</th>\n",
       "      <th>last_event_to_snapshot_gap</th>\n",
       "      <th>last_session_time_hod</th>\n",
       "      <th>last_session_time_dow</th>\n",
       "      <th>email_domain</th>\n",
       "      <th>email_ext</th>\n",
       "      <th>rfm_label</th>\n",
       "      <th>recency_score</th>\n",
       "      <th>frequency_score</th>\n",
       "      <th>monetary_value_score</th>\n",
       "      <th>user_lifespan</th>\n",
       "      <th>cumulative_order_num</th>\n",
       "      <th>cumulative_revenue</th>\n",
       "      <th>event_counts</th>\n",
       "      <th>mean_gap</th>\n",
       "      <th>last_campaign_name_cleaned</th>\n",
       "      <th>last_order_locationid_categorical</th>\n",
       "      <th>order_lifespan_bin_combo</th>\n",
       "      <th>event_name_transformed</th>\n",
       "      <th>event_time_day_of_week</th>\n",
       "      <th>event_time_hour_of_day</th>\n",
       "      <th>fake_customerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>lifecycle register to purchase lifecycle regis...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>gmail</td>\n",
       "      <td>com</td>\n",
       "      <td>00_Lost</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>26.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>2.091667</td>\n",
       "      <td>lifecycle register to purchase</td>\n",
       "      <td>LOC: 19</td>\n",
       "      <td>0 | 2</td>\n",
       "      <td>email</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>t non fam free delivery</td>\n",
       "      <td>59.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>gmail</td>\n",
       "      <td>com</td>\n",
       "      <td>06_Promising</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>450</td>\n",
       "      <td>2</td>\n",
       "      <td>16.340000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t4 non fam free delivery</td>\n",
       "      <td>LOC: 182</td>\n",
       "      <td>1 | 5</td>\n",
       "      <td>push</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>days of deals day new-act-dorm ncoup new years...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>gmail</td>\n",
       "      <td>com</td>\n",
       "      <td>02_At_Risk</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>6</td>\n",
       "      <td>92.940002</td>\n",
       "      <td>14</td>\n",
       "      <td>3.862180</td>\n",
       "      <td>valentines day launch chocolatesroses no alc m...</td>\n",
       "      <td>LOC: 106</td>\n",
       "      <td>3 | 4</td>\n",
       "      <td>email</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>lifecycle register to purchase lifecycle regis...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>gmail</td>\n",
       "      <td>com</td>\n",
       "      <td>06_Promising</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>10.360000</td>\n",
       "      <td>10</td>\n",
       "      <td>1.379630</td>\n",
       "      <td>lifecycle 1st to 2nd</td>\n",
       "      <td>LOC: 106</td>\n",
       "      <td>0 | 2</td>\n",
       "      <td>email</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail</td>\n",
       "      <td>com</td>\n",
       "      <td>neverbuyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOC: UNKNOWN</td>\n",
       "      <td>0 | 4</td>\n",
       "      <td>email</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 campaign_concat_str  \\\n",
       "0  lifecycle register to purchase lifecycle regis...   \n",
       "1                            t non fam free delivery   \n",
       "2  days of deals day new-act-dorm ncoup new years...   \n",
       "3  lifecycle register to purchase lifecycle regis...   \n",
       "4                                                NaN   \n",
       "\n",
       "   last_event_to_snapshot_gap  last_session_time_hod  last_session_time_dow  \\\n",
       "0                         5.0                   18.0                    5.0   \n",
       "1                        59.0                   14.0                    0.0   \n",
       "2                         3.0                    4.0                    4.0   \n",
       "3                         1.0                    4.0                    6.0   \n",
       "4                         NaN                    NaN                    NaN   \n",
       "\n",
       "  email_domain email_ext     rfm_label  recency_score  frequency_score  \\\n",
       "0        gmail       com       00_Lost              0                0   \n",
       "1        gmail       com  06_Promising              4                0   \n",
       "2        gmail       com    02_At_Risk              1                2   \n",
       "3        gmail       com  06_Promising              4                0   \n",
       "4        gmail       com    neverbuyer              0                0   \n",
       "\n",
       "   monetary_value_score  user_lifespan  cumulative_order_num  \\\n",
       "0                     0             17                     1   \n",
       "1                     2            450                     2   \n",
       "2                     2            211                     6   \n",
       "3                     2             16                     1   \n",
       "4                     0            155                     0   \n",
       "\n",
       "   cumulative_revenue  event_counts  mean_gap  \\\n",
       "0           26.500000             6  2.091667   \n",
       "1           16.340000             1       NaN   \n",
       "2           92.940002            14  3.862180   \n",
       "3           10.360000            10  1.379630   \n",
       "4            0.000000             0       NaN   \n",
       "\n",
       "                          last_campaign_name_cleaned  \\\n",
       "0                     lifecycle register to purchase   \n",
       "1                           t4 non fam free delivery   \n",
       "2  valentines day launch chocolatesroses no alc m...   \n",
       "3                               lifecycle 1st to 2nd   \n",
       "4                                                NaN   \n",
       "\n",
       "  last_order_locationid_categorical order_lifespan_bin_combo  \\\n",
       "0                           LOC: 19                    0 | 2   \n",
       "1                          LOC: 182                    1 | 5   \n",
       "2                          LOC: 106                    3 | 4   \n",
       "3                          LOC: 106                    0 | 2   \n",
       "4                      LOC: UNKNOWN                    0 | 4   \n",
       "\n",
       "  event_name_transformed  event_time_day_of_week  event_time_hour_of_day  \\\n",
       "0                  email                       7                      18   \n",
       "1                   push                       4                      23   \n",
       "2                  email                       4                      23   \n",
       "3                  email                       7                       6   \n",
       "4                  email                       4                       0   \n",
       "\n",
       "   fake_customerid  \n",
       "0                0  \n",
       "1                1  \n",
       "2                2  \n",
       "3                3  \n",
       "4                4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "## Optimizer App Method\n",
    "\n",
    "With this method, we \n",
    "1. cache a .csv file to our disk\n",
    "2. call the rest API to the optimizer that is already setup\n",
    "3. wait for results to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization job created. Dataset ID: 5ee21943c51c3b00018f5696\n",
      "Waiting for optimization results...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n",
      "Dataset 5ee21943c51c3b00018f5696 is not ready yet. Waiting...\n"
     ]
    }
   ],
   "source": [
    "future_cols = ['event_name_transformed','event_time_day_of_week','event_time_hour_of_day'] \n",
    "\n",
    "# optimizer app wants to see blank columns\n",
    "for c in future_cols:\n",
    "    df[c]=''\n",
    "\n",
    "# optimizer app works by caching a file to disk and then sending it\n",
    "df.to_csv(Path(data_dir) / FILENAME_OPT_CACHE, index=False)\n",
    "\n",
    "############# Using retry request #############\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "ATTEMPT_COUNT = 1024\n",
    "ATTEMPT_INCREMENT = 60\n",
    "\n",
    "class MaxAttemptsLimitReached(Exception):\n",
    "    pass\n",
    "\n",
    "class retry_on_false:\n",
    "    \"\"\"\n",
    "    Helper decorator class to retry getting data from resource before give up.\n",
    "    \"\"\"\n",
    "    def __init__(self, attempts_count: int = 20, time_wait: int = 60) -> None:\n",
    "        self.attempts_count = ATTEMPT_COUNT\n",
    "        self.time_wait = ATTEMPT_INCREMENT  # sec\n",
    "    def __call__(self, func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            i = 0\n",
    "            while i <= self.attempts_count:\n",
    "                results = func(*args, **kwargs)\n",
    "                if results:\n",
    "                    return results\n",
    "                else:\n",
    "                    time.sleep(self.time_wait)\n",
    "                    i = i + 1\n",
    "                    continue\n",
    "            raise MaxAttemptsLimitReached(\"Maximum attempts limit reached.\")\n",
    "            \n",
    "        return wrapper\n",
    "    \n",
    "def send_dataset(path):\n",
    "    with open(path) as f:\n",
    "        response = requests.post(f\"{app_url}/api/uploadAsync\", files={'file': f}, headers=headers, timeout=(5, 60))\n",
    "        if response.status_code > 400:\n",
    "            raise Exception(response.content)\n",
    "        data = response.json()\n",
    "        if response.status_code == 400:\n",
    "            raise Exception('\\n'.join(data['errors']))\n",
    "            \n",
    "        return data['dataset']\n",
    "    \n",
    "@retry_on_false(attempts_count=ATTEMPT_COUNT) \n",
    "def wait_dataset_readiness(dataset_id):\n",
    "    response = requests.get(f\"{app_url}/api/datasets/{dataset_id}\", headers=headers, timeout=(3, ATTEMPT_INCREMENT))\n",
    "    response.raise_for_status()\n",
    "    dataset_info = response.json()\n",
    "    if dataset_info['task']['status'] in ('CANCELED', 'ERRORED', 'FINISHED'):\n",
    "        return dataset_info\n",
    "    print(f\"Dataset {dataset_id} is not ready yet. Waiting...\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "def download_optimization_results(dataset_id):\n",
    "    response = requests.get(f\"{app_url}/api/datasets/{dataset_id}/download\", headers=headers)\n",
    "    response.raise_for_status()\n",
    "    path = str(Path(data_dir) / FILENAME_OPT_RESULT)\n",
    "    with open(path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                time.sleep(1)\n",
    "                \n",
    "    return path\n",
    "\n",
    "def optimize_dataset(path):\n",
    "    dataset = send_dataset(path)\n",
    "    print(f\"Optimization job created. Dataset ID: {dataset['id']}\")\n",
    "    print(\"Waiting for optimization results...\")\n",
    "    dataset = wait_dataset_readiness(dataset['id'])\n",
    "    if dataset['task']['status'] in ('ERRORED', 'CANCELED'):\n",
    "        print(dataset['task'].get('error_message'))\n",
    "        return\n",
    "    print(\"Optimization finished. Downloading...\")\n",
    "    results_path = download_optimization_results(dataset['id'])\n",
    "    print(f\"Finished downloading optimizations into: {results_path}\")\n",
    "    \n",
    "    return results_path\n",
    "\n",
    "\n",
    "start = time.monotonic()\n",
    "optimize_dataset(Path(data_dir) / FILENAME_OPT_CACHE)\n",
    "print('TIME: ', round((time.monotonic() - start) / 60), 'Min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "Hyperopt is not designed for this type of problem, but it yields the best results (strangely):\n",
    "- 1K  : 28 min\n",
    "- 10K : 284 min\n",
    "\n",
    "Grid Search is ideal for this problem since there are only 3 variables to optimize and only 1,512 possibilities per row, but this actually took slower:\n",
    "- 1K  : 512 min (est) would not finish\n",
    "- 10K : 20,480 minutes (est) not realistically possible\n",
    "\n",
    "This execution time is concerning when dealing with potentially bigger data. We assume that this will continue to scale linearly with the increase in file size, because by watching the results appear through the Optimizer UI, we can see the progress incrementing with each record that is solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_opt = pd.read_csv(Path(data_dir) / FILENAME_OPT_RESULT)\n",
    "result_opt.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "## Brute Force Method\n",
    "To compare the execution time, we will try a brute force method which should try every possible combination and then take the maximum:\n",
    "1. Create cartesian product with all possible combinations\n",
    "2. Cache file to disk\n",
    "3. Submit for scoring against deployment and download results\n",
    "4. Group rank to isolate maximum choice for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME:  5 Min\n"
     ]
    }
   ],
   "source": [
    "df = indata.copy()   \n",
    "df = pd.concat([df]*times_to_repeat, ignore_index=True)\n",
    "df['fake_customerid'] = df.index\n",
    "df.drop(columns=['has_session_w_atc','z_cost'], inplace=True)\n",
    "\n",
    "future_cols = ['event_name_transformed','event_time_day_of_week','event_time_hour_of_day'] \n",
    "df.drop(columns=future_cols, inplace=True)\n",
    "\n",
    "import time\n",
    "start = time.monotonic()\n",
    "\n",
    "# helper function\n",
    "def create_list_from_range(r1,r2): \n",
    "    if (r1 == r2): \n",
    "        return r1 \n",
    "    else: \n",
    "        res = [] \n",
    "        while(r1 < r2+1 ): \n",
    "            res.append(r1) \n",
    "            r1 += 1\n",
    "        return res\n",
    "\n",
    "# make a list of options for the 3 columns\n",
    "options_event_name_transformed = ['email','push','webhook','email+push','email+push+push','email+push+push+push','email+webhook','push+push','push+push+push']\n",
    "options_event_time_day_of_week = create_list_from_range(1,7)\n",
    "options_event_time_hour_of_day = create_list_from_range(0,23)\n",
    "\n",
    "# turn each list into a dataframe\n",
    "df_options = pd.DataFrame({'event_name_transformed': options_event_name_transformed})\n",
    "df_day = pd.DataFrame({'event_time_day_of_week': options_event_time_day_of_week})\n",
    "df_hour = pd.DataFrame({'event_time_hour_of_day': options_event_time_hour_of_day})\n",
    "\n",
    "# add a dummy columns to everything, because pythons stupid\n",
    "df_options['dummy']=1\n",
    "df_day['dummy']=1\n",
    "df_hour['dummy']=1\n",
    "df['dummy'] = 1\n",
    "\n",
    "# now cartesian product... cascading, because pythons stupid\n",
    "dfDayWeek = pd.merge(df_day, df_hour, on='dummy')\n",
    "dfDayWeekOpt = pd.merge(dfDayWeek, df_options, on='dummy')\n",
    "dfToScore = pd.merge(df[df.columns[~df.columns.isin(['event_name_transformed','event_time_day_of_week','event_time_hour_of_day'])]], dfDayWeekOpt, on='dummy')\n",
    "\n",
    "# cache to disk for a fair comparison\n",
    "dfToScore.to_csv(Path(data_dir) / FILENAME_BRUTE_CACHE, index=False)\n",
    "\n",
    "# now submit for scoring | https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.20.0/entities/batch_predictions.html\n",
    "\n",
    "#######################################################################\n",
    "infile = str(Path(data_dir) / FILENAME_BRUTE_CACHE)\n",
    "outfile = str(Path(data_dir) / FILENAME_BRUTE_RESULT)\n",
    "\n",
    "import datarobot as dr\n",
    "dr.Client(endpoint=ENDPOINT_URL, token=API_TOKEN, connect_timeout=9999, max_retries=99)\n",
    "\n",
    "dr.BatchPredictionJob.score_to_file(deployment_id,infile,outfile, passthrough_columns=['fake_customerid','event_name_transformed','event_time_day_of_week','event_time_hour_of_day'])\n",
    "\n",
    "scored = pd.read_csv(Path(data_dir) / FILENAME_BRUTE_RESULT)\n",
    "scored['rank'] = scored.groupby('fake_customerid')['has_session_w_atc_1.0_PREDICTION'].rank(ascending=False, method='first')\n",
    "\n",
    "result_brute = scored[scored['rank']==1]\n",
    "\n",
    "print('TIME: ', round((time.monotonic() - start) / 60), 'Min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "(Max options (24 * 7 * 9) = 1512 row expansion)\n",
    "- 1K  : 4 min\n",
    "- 10K : 37 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_brute.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "## Comparing Accuracy of Results\n",
    "\n",
    "Here we can see that the brute force method is not only faster, but more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_brute['has_session_w_atc_1.0_PREDICTION'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_opt['optimized_prediction'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disagreements\n",
    "Let's filter for records where the optimum choice disagrees between both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result_brute = result_brute.set_index(['fake_customerid'])\n",
    "    result_opt = result_opt.set_index(['fake_customerid'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "joined = pd.merge(result_brute[['has_session_w_atc_1.0_PREDICTION']], result_opt[['optimized_prediction']], left_index=True, right_index=True)\n",
    "\n",
    "joined['absdiff'] = np.abs(joined['has_session_w_atc_1.0_PREDICTION'] - joined['optimized_prediction'])\n",
    "\n",
    "disagrees = joined[joined['absdiff']>0.000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_opt = result_opt.loc[disagrees.index][['optimized_event_time_day_of_week','optimized_event_time_hour_of_day','optimized_event_name_transformed','optimized_prediction']]\n",
    "disagree_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing on the first one, fake_customerid=1, we can see that the 9th most optimal value was chosen by the optimizer tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_brute = scored[scored['fake_customerid'].isin(disagrees.index)].sort_values('fake_customerid').sort_values(['fake_customerid','rank'])\n",
    "disagree_brute[disagree_brute['fake_customerid']==1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "When it comes to scaling this to millions of customers, we're stuck between a rock and a hard place.\n",
    "\n",
    "#### Optimization App\n",
    "*Pros* \n",
    " - Only sending records as-is (send 1 row per customer)\n",
    "\n",
    "*Cons* \n",
    " - Not always optimum (this is a minor difference, however)\n",
    " - Will take too long to execute (10M rows => 286,000 minutes => 4,766 hours => 199 days\n",
    " \n",
    "#### Brute Force\n",
    "*Pros*\n",
    " - Answer will be optimum\n",
    " - More efficient in calculations per row\n",
    "\n",
    "*Cons* \n",
    " - Need to pre-populate every possible choice: So, we'll quickly be limited by memory and/or disk space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "Is there a way that we can intelligently cache the results to disk without exceeding memory limits, performing the Brute Force operation in chunks?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
