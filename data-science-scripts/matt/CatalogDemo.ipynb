{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import datarobot as dr\n",
    "from datetime import date\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import io\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_collection = {}\n",
    "\n",
    "def createDFarray(df, size_threshold, part_number=0):\n",
    "    # play around with size and threshold to get the right size, ultimately it needs to be less than 1GB\n",
    "    file_size = df.memory_usage(index=True, deep=False).sum() / 1038336\n",
    "    print(file_size)\n",
    "    num_records = len(df)\n",
    "\n",
    "    if file_size > size_threshold:\n",
    "        records_to_split_off = int(num_records * size_threshold // file_size)\n",
    "        df_to_save = df.head(records_to_split_off)\n",
    "        dataframe_collection[part_number] = pd.DataFrame(df_to_save)\n",
    "        createDFarray(df.tail(num_records-records_to_split_off), size_threshold, part_number=part_number+1)\n",
    "\n",
    "    else:\n",
    "        dataframe_collection[part_number] = pd.DataFrame(df)\n",
    "        return dataframe_collection\n",
    "\n",
    "def wait_for_async_resolution(client, status_url):\n",
    "        status = False\n",
    "        while status == False:\n",
    "            resp = client.get(status_url)\n",
    "            r = json.loads(resp.content)\n",
    "            try:\n",
    "                statusjob = r['status'].upper()\n",
    "                \n",
    "            except:\n",
    "                statusjob = ''\n",
    "            if resp.status_code == 200 and statusjob != 'RUNNING' and statusjob != 'INITIALIZED': \n",
    "                status = True\n",
    "                return resp\n",
    "            time.sleep(10)   # Delays for 10 seconds.\n",
    "\n",
    "def wait_for_result(client, response):\n",
    "    assert response.status_code in (200, 201, 202), response.content\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "    elif response.status_code == 201:\n",
    "        status_url = response.headers['Location']\n",
    "        resp = client.get(status_url)\n",
    "        assert resp.status_code == 200, resp.content\n",
    "        data = resp.json()\n",
    "\n",
    "    elif response.status_code == 202:\n",
    "        status_url = response.headers['Location']\n",
    "        resp = wait_for_async_resolution(client, status_url)\n",
    "        data = resp.json()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingdata file\n",
    "# https://dataset-for-performance-testing.s3.amazonaws.com/airlines_5gb.csv\n",
    "FILENAME = '../SampleData/airlines_5gb.csv'\n",
    "\n",
    "# setup your headers for the calls to the app and prediction server\n",
    "API_TOKEN = '<YOUR TOKEN>'\n",
    "USERNAME = 'felix.huthmacher@datarobot.com'\n",
    "MODELMANAGEMENTENDPOINT = 'https://app.datarobot.com/api/v2'\n",
    "MODELMANAGEMENTHEADERS = {'Authorization': 'token %s' % API_TOKEN }\n",
    "DATAMANAGEMENTHEADERS = {'Content-Type': 'application/json; charset=UTF-8', 'Authorization': 'token %s' % API_TOKEN }\n",
    "\n",
    "drclient = dr.Client(endpoint=MODELMANAGEMENTENDPOINT, token=API_TOKEN,connect_timeout=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect_timeout is in seconds\n",
    "drclient = dr.Client(endpoint=MODELMANAGEMENTENDPOINT, token=API_TOKEN, connect_timeout=1000)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET = 'ArrDelay'\n",
    "PROJECTNAME = 'DataSetScoringDemo '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_AUTOPILOT = False\n",
    "USE_EXISTING_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. upload training data to catalog\n",
    "if (USE_EXISTING_MODEL == False):     \n",
    "    \n",
    "    payload = {\n",
    "        'description': 'AI Catalog Demo Project',\n",
    "        'array categories': \"TRAINING\",\n",
    "        'file': ('Airlines Dataset 5gb 4', open(FILENAME, 'r')),\n",
    "        'max_wait': 1800\n",
    "    }\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    #response = drclient.build_request_with_file(\"POST\",\n",
    "    #                                           \"%s/datasets/fromFile/\" % (MODELMANAGEMENTENDPOINT),\n",
    "    #                                           \"AI Catalog Demo Project\",\n",
    "    #                                           file_path=FILENAME,\n",
    "    #                                           read_timeout=1200\n",
    "    #                                          )\n",
    "    response = requests.post(\"%s/datasets/fromFile/\" % (MODELMANAGEMENTENDPOINT),\n",
    "                            headers=MODELMANAGEMENTHEADERS,  files = payload, timeout = 1800 )\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    print (response)\n",
    "    \n",
    "    # wait till file is registered in the catalog\n",
    "    # registering a 5gb dataset takes about \n",
    "    datacatalog_response = wait_for_result(drclient, response)\n",
    "\n",
    "    # get dataset details\n",
    "    dataset_response = requests.get(\"%s/datasets/%s/\" % (MODELMANAGEMENTENDPOINT, datacatalog_response['datasetId']),\n",
    "                            headers=DATAMANAGEMENTHEADERS)\n",
    "    dataset = dataset_response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create a new project with data from catalog\n",
    "if (USE_EXISTING_MODEL == False):\n",
    "    payload = {\n",
    "        'projectName': 'AI Catalog Demo Project ' + str(date.today()),\n",
    "        'datasetId': str(dataset['datasetId']),\n",
    "        'datasetVersionId': str(dataset['versionId']),\n",
    "        'user': USERNAME,\n",
    "        'password': API_TOKEN\n",
    "    }\n",
    "    project_response = drclient.post(\n",
    "        '/projects/',\n",
    "        data=payload,\n",
    "        headers={'Content-Type': 'application/json'}\n",
    "    )    \n",
    "\n",
    "    print(project_response)\n",
    "    projectID = project_response.json()['pid']\n",
    "    # wait till project is created\n",
    "    project_response = wait_for_result(drclient, project_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a. Creates new featurelist for each project as needed\n",
    "\n",
    "if (USE_EXISTING_MODEL == False):\n",
    "    newProject = dr.Project(projectID)\n",
    "    feature_names = []\n",
    "    # get all features from dataset\n",
    "    # Substracts lists by converting them into sets (order not preserved)\n",
    "    for i in newProject.get_features():\n",
    "        feature_names.append(i.name)\n",
    "    \n",
    "    def list_diff(li1, li2):\n",
    "        return (list(set(li1) - set(li2)))\n",
    "\n",
    "    # 2b. create new featurelist by removing the below features from the featurelist\n",
    "    unwanted_features = ['TaxiIn', 'TaxiOut', 'TailNum']\n",
    "    project_featurelist = list_diff(feature_names, unwanted_features)\n",
    "    newFeatureList = newProject.create_featurelist(\"featurelist_500_01\", project_featurelist)\n",
    "    print(newFeatureList.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (USE_AUTOPILOT == True) & (USE_EXISTING_MODEL == False):\n",
    "    newProject.set_target(target=TARGET,\n",
    "                       mode=dr.AUTOPILOT_MODE.QUICK,\n",
    "                       worker_count= 8,\n",
    "                       featurelist_id=newFeatureList.id,\n",
    "                       max_wait= 370\n",
    "                       )\n",
    "    newProject.wait_for_autopilot()\n",
    "    recommendation_type = dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT\n",
    "    recommendation = dr.models.ModelRecommendation.get(newProject.id, recommendation_type)\n",
    "    bestModelId = recommendation.model_id\n",
    "    newProjectId = newProject.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (USE_AUTOPILOT == False) & (USE_EXISTING_MODEL == False):\n",
    "    newProject.set_target(target=TARGET,\n",
    "                       mode=dr.AUTOPILOT_MODE.MANUAL,\n",
    "                       worker_count= 8,\n",
    "                       featurelist_id=newFeatureList.id,\n",
    "                       max_wait= 36000000\n",
    "                       )\n",
    "\n",
    "    # pick any blueprint/model from repository\n",
    "    blueprints = newProject.get_blueprints()\n",
    "    for blueprint in blueprints:\n",
    "        if blueprint.model_type == 'RuleFit Regressor':\n",
    "            bestblueprint = blueprint\n",
    "            break\n",
    "\n",
    "    JobId = newProject.train(bestblueprint, sample_pct=50)\n",
    "    newModel = dr.models.modeljob.wait_for_async_model_creation(project_id=newProject.id, model_job_id=JobId)\n",
    "    fi = newModel.get_or_request_feature_impact(600)\n",
    "    newModel.cross_validate()\n",
    "    bestModelId = newModel.id\n",
    "    newProjectId = newProject.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (USE_EXISTING_MODEL == True):\n",
    "    \n",
    "    newProjectId = '5dca127a7be86909a62c1670' \n",
    "    bestModelId = '5dca4160c0a77c4af806bb85'\n",
    "    dataset = json.loads('{ \"datasetId\":\"5dc9e8acbaa3940735339cd7\", \"versionId\":\"5dc9e8acbaa3940735339cdc\"}')\n",
    "    print(dataset['datasetId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. score training data with a model from the project\n",
    "model = dr.Model.get(model_id=bestModelId,project=newProjectId)\n",
    "pred_job = model.request_training_predictions(dr.enums.DATA_SUBSET.ALL)\n",
    "MAX_WAIT = 60 * 60  # Maximum number of seconds to wait for prediction job to finish\n",
    "predictions = pred_job.get_result_when_complete(max_wait=MAX_WAIT)\n",
    "predictions.to_csv('trainingdata_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. score additional data with a model from the project\n",
    "\n",
    "# check size of scoringdata\n",
    "# each <1GB dataframe for prediction data\n",
    "\n",
    "# just for demo purposes a subset of the data\n",
    "# <1GB for scoring data upload to DR\n",
    "scoringdata = pd.read_csv(FILENAME,nrows=10000,encoding=\"ISO-8859-1\")\n",
    "\n",
    "dataframe_collection = createDFarray(scoringdata, 0.1, part_number=0)\n",
    "\n",
    "for key in dataframe_collection.keys():\n",
    "    \n",
    "    csv_string = dataframe_collection[key].to_csv()\n",
    "    csv_string = str.encode(csv_string)\n",
    "    payload = {\n",
    "       'description': 'testCatalogScoring ' + str(key),\n",
    "       'file': (io.BytesIO(csv_string))\n",
    "    }\n",
    "    response = requests.post(\"%s/datasets/fromFile/\" % (MODELMANAGEMENTENDPOINT),\n",
    "                            headers=MODELMANAGEMENTHEADERS,  files = payload )\n",
    "\n",
    "    # wait till file is uploaded to catalog\n",
    "    datacatalog_response = wait_for_result(drclient, response)\n",
    "\n",
    "    # get dataset details\n",
    "    dataset_response = requests.get(\"%s/datasets/%s/\" % (MODELMANAGEMENTENDPOINT, datacatalog_response['datasetId']),\n",
    "                            headers=MODELMANAGEMENTHEADERS)\n",
    "    dataset = dataset_response.json()\n",
    "    \n",
    "    # Retrieves the details of the dataset with given ID and version ID\n",
    "    # /api/v2/datasets/(datasetId)/versions/(datasetVersionId)/\n",
    "    #dataset_response = requests.get(\"%s/datasets/%s/versions/%s/\" % (MODELMANAGEMENTENDPOINT, dataset['datasetId'],dataset['versionId']),\n",
    "    #                        headers=MODELMANAGEMENTHEADERS)\n",
    "    #dataset = dataset_response.json()\n",
    "\n",
    "    # link dataset to project & model\n",
    "    payload = {\n",
    "        'datasetId': str(dataset['datasetId']),\n",
    "        'datasetVersionId': str(dataset['versionId']) \n",
    "    }\n",
    "\n",
    "    response_assoc = drclient.post(\n",
    "        '/projects/%s/predictionDatasets/datasetUploads/' % (newProjectId),\n",
    "        data=payload,\n",
    "        headers={'Content-Type': 'application/json'}\n",
    "    )                          \n",
    "    predictiondata = response_assoc.json()\n",
    "\n",
    "    # score data\n",
    "    model = dr.Model.get(model_id=bestModelId,project=newProjectId)\n",
    "    pred_job = model.request_predictions(predictiondata['datasetId'])\n",
    "    MAX_WAIT = 60 * 60  # Maximum number of seconds to wait for prediction job to finish\n",
    "    predictions = pred_job.get_result_when_complete(max_wait=MAX_WAIT)\n",
    "    predictions.to_csv('scoringdata_results.csv')\n",
    "    for row in predictions.iterrows():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
