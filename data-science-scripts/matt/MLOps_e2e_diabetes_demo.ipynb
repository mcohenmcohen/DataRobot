{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# End-to-End Model Management & Monitoring Demo\n",
    "\n",
    "In this demo we cover the beginning-to-end process of:\n",
    "\n",
    "1. Creating a deployment from a DataRobot model\n",
    "2. Submitting prediction requests against the Deployment\n",
    "3. Feeding back \"Actuals\" to the Deployment\n",
    "4. Retrieving monitoring stats about the Deployment\n",
    "5. Replacing the Model behind the Deployment and make new predictions against it\n",
    "\n",
    "All of these steps are done via the API, but at any time you can view the progress in the DataRobot UI or replace one of these programmtic steps with manual clicks in the UI. \n",
    "\n",
    "Be sure to also follow along via the **[5-part video tutorial series](https://drive.google.com/open?id=1vohFClc1x-ieEbLELMFJewzDPqbTmdlG)**!\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "#### A) Setup\n",
    "        1. Credentials\n",
    "        2. Constants\n",
    "#### B) Creating a Deployment\n",
    "        1. Create Project\n",
    "        2. Run Autopilot\n",
    "        3. Select Model\n",
    "        4. Deploy Model\n",
    "        5. Configure Deployment\n",
    "#### C) Submit Prediction Requests & Feed Back Actuals\n",
    "        1. Prepare Predictions & Actuals Data\n",
    "        2. Submit Prediction Requests\n",
    "        3. Feed Back Actuals\n",
    "#### D) Monitor Your Deployment\n",
    "        1. Retrieve Service Stats\n",
    "        2. Retrieve Accuracy Stats\n",
    "#### E) Replace Your Model\n",
    "        1. Select an Alternative Model\n",
    "        2. Validate the Alternative Model\n",
    "        3. Replace the Model\n",
    "        4. Submit predictions against the replaced model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A) Setup\n",
    "\n",
    "In this section we need to authenticate our DataRobot API client and specify some constants that will be used throughout this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERNAME: matthew.cohen@datarobot.com\n",
      "DATAROBOT_API_KEY: NWQ1NDA3ZTdmNTU1Y2QxZDQxNmZjZTYxOmFHZmZ1MlBhcUVQSHY5bzhWTjk3V05qcXBLVEpadC1R\n",
      "DATAROBOT_KEY: 544ec55f-61bf-f6ee-0caf-15c7f919a45d\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: Set up your credentials and instantiate the DataRobot API Client.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "import datarobot as dr\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datarobot.enums import SERVICE_STAT_METRIC\n",
    "\n",
    "USERNAME=os.getenv('DATAROBOT_USERNAME')\n",
    "DATAROBOT_API_KEY=os.getenv('DATAROBOT_API_TOKEN')\n",
    "DATAROBOT_KEY=os.getenv('DATAROBOT_KEY')\n",
    "\n",
    "# print('USERNAME:', USERNAME)\n",
    "# print('DATAROBOT_API_KEY:', DATAROBOT_API_KEY)\n",
    "# print('DATAROBOT_KEY:', DATAROBOT_KEY)\n",
    "\n",
    "CLEAR_SECRETS = False  # Set to True if you'd like to clear your credentials\n",
    "BASE_API_URL = 'https://app.datarobot.com/api/v2'\n",
    "PREDICTION_SERVER_BASE_URL =  'https://cfds-ccm-prod.orm.datarobot.com/' # 'https://datarobot-predictions.orm.datarobot.com'\n",
    "\n",
    "dr.Client(token=DATAROBOT_API_KEY, endpoint=BASE_API_URL)\n",
    "\n",
    "# We will also create some functions for accessing the API directly because these routes are\n",
    "# not yet exposed by the DataRobot Python SDK.\n",
    "HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization':  'Token {}'.format(DATAROBOT_API_KEY),\n",
    "}\n",
    "\n",
    "def set_association_id(deployment_id, association_id, allow_missing_values=False):\n",
    "    \"\"\"Assigns the association ID for a deployment\"\"\"\n",
    "    url = f'{BASE_API_URL}/deployments/{deployment_id}/settings/'\n",
    "    \n",
    "    data = {\n",
    "        'associationId': {'requiredInPredictionRequests': not allow_missing_values, 'columnNames': [association_id]}\n",
    "    }\n",
    "    \n",
    "    resp = requests.patch(url, json=data, headers=HEADERS)\n",
    "    resp.raise_for_status()\n",
    "    return\n",
    "\n",
    "\n",
    "def spoof_deployment_start_date(deployment_id, start_date):\n",
    "    \"\"\"An internal API used for back-dating when a deployment started.\"\"\"\n",
    "    url = f'{BASE_API_URL}/deployments/{deployment_id}/modelHistory/currentModel/'\n",
    "    \n",
    "    data = {'startDate': start_date}\n",
    "    \n",
    "    resp = requests.patch(url, json=data, headers=HEADERS)\n",
    "    resp.raise_for_status()\n",
    "    return\n",
    "\n",
    "\n",
    "def make_deployment_predictions(deployment_id, data, prediction_time=None):\n",
    "    \"\"\"\n",
    "    Make predictions against a deployment.\n",
    "    \"\"\"\n",
    "    headers = {**HEADERS, 'datarobot-key': DATAROBOT_KEY}\n",
    "    if prediction_time:\n",
    "        headers['X-DataRobot-Prediction-Timestamp'] = prediction_time.isoformat()\n",
    "\n",
    "    ## Note that this uses a different base url than other API enndpoints.\n",
    "    url = f'{PREDICTION_SERVER_BASE_URL}/predApi/v1.0/deployments/{deployment_id}/predictions'\n",
    "\n",
    "    resp = requests.post(\n",
    "        url,\n",
    "        auth=(USERNAME, DATAROBOT_API_KEY),\n",
    "        data=data,\n",
    "        headers=headers\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 2: Specify constants that will be used to configure your project, model, and deployment.\n",
    "\"\"\"\n",
    "\n",
    "# Project Info\n",
    "project_name = 'MMM E2E Tutorial'\n",
    "worker_count = 4\n",
    "\n",
    "# Training Data\n",
    "dataset_url = 'https://s3.amazonaws.com/datarobot-public-datasets-redistributable/10k_diabetes_20.xlsx'\n",
    "target_name = 'readmitted'\n",
    "\n",
    "# Modeling\n",
    "recommendation_type = dr.enums.RECOMMENDED_MODEL_TYPE.RECOMMENDED_FOR_DEPLOYMENT\n",
    "\n",
    "# Deployment\n",
    "deployment_label = project_name\n",
    "deployment_description = 'Used to practice the end-to-end process of creating and managing a deployment.'\n",
    "association_id_column_name = 'event_id'\n",
    "\n",
    "# Predictions\n",
    "prediction_dataset_url = 'https://s3.amazonaws.com/datarobot_public_datasets/exploratory_testing/mmm_accuracy/10k_predictions_alphatesting_associd.csv'\n",
    "prediction_sample_size = 1000  # The number of rows to randomly sample from the above files and submit for predictions.\n",
    "past_n_days = 365  # We will spoof when our predictions were submitted, going back this many days until today.\n",
    "num_prediction_chunks = 100  # We will spread out our predictions into this many chunks across the above time range.\n",
    "\n",
    "# Actuals\n",
    "date_format = '%A %-d, %Y'\n",
    "actuals_dataset_url = 'https://s3.amazonaws.com/datarobot_public_datasets/exploratory_testing/mmm_accuracy/10k_actuals_alphatesting.csv'\n",
    "\n",
    "# Model Monitoring\n",
    "monitoring_window_start=(datetime.now() - timedelta(days=past_n_days)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "monitoring_window_end=datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# Model Replacement\n",
    "model_replacement_recommendation_type = dr.enums.RECOMMENDED_MODEL_TYPE.MOST_ACCURATE\n",
    "model_replacement_reason = dr.enums.MODEL_REPLACEMENT_REASON.ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## B) Creating a Deployment\n",
    "\n",
    "Here we create a new DataRobot project, run Autopilot, select a recommended model, create a Deployment from it, and\n",
    "set crucial Deployment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project \"MMM E2E Tutorial\" with ID 5e920d5590f0540f2418712a\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: Find an existing project by the specifed name or create one if needed.\n",
    "\"\"\"\n",
    "projects = dr.Project.list(search_params={'project_name': project_name})\n",
    "\n",
    "if not projects:\n",
    "    project = dr.Project.create(dataset_url, project_name)\n",
    "else:\n",
    "    project = projects[0]\n",
    "\n",
    "print(f'Using project \"{project.project_name}\" with ID {project.id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target has already been set to `readmitted`.\n",
      "Autopilot is complete and we are ready to select a model to deploy.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 2: Set our target, kick off autopilot, and wait for it to complete.\n",
    "\"\"\"\n",
    "try:\n",
    "    project.set_target(\n",
    "        target=target_name,\n",
    "        mode=dr.AUTOPILOT_MODE.FULL_AUTO,\n",
    "        worker_count=worker_count,\n",
    "    )\n",
    "except dr.errors.ClientError as err:\n",
    "    if err.status_code == 422:\n",
    "        print(f'Target has already been set to `{target_name}`.')\n",
    "    else:\n",
    "        raise\n",
    "else:\n",
    "    print(f'The target has been set to `{target_name}`')\n",
    "    project.wait_for_autopilot(verbosity=dr.VERBOSITY_LEVEL.SILENT)\n",
    "\n",
    "print('Autopilot is complete and we are ready to select a model to deploy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataRobot recommends a RandomForest Classifier (Gini) model with ID 5e9210b0e5620f181e6aeb2a.\n",
      "Using a RandomForest Classifier (Gini) model with ID 5e9210b0e5620f181e6aeb2a.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3: Identify a model to deploy\n",
    "\"\"\"\n",
    "use_recommended_model = True\n",
    "fallback_model_id = None\n",
    "\n",
    "if use_recommended_model:\n",
    "    recommended_model = dr.ModelRecommendation.get(project.id, recommendation_type)\n",
    "    model = dr.Model.get(recommended_model.project_id, recommended_model.model_id)\n",
    "    print(f'DataRobot recommends a {model.model_type} model with ID {model.id}.')\n",
    "elif fallback_model_id:\n",
    "    model = dr.Model.get(project.id, fallback_model_id)\n",
    "else:\n",
    "    raise RuntimeError('You must specify a fallback_model_id if you choose not to use a recommended model.')\n",
    "\n",
    "print(f'Using a {model.model_type} model with ID {model.id}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No deployment exists for this project/model yet. Creating one now.\n",
      "Using Deployment with label \"MMM E2E Tutorial\" and ID 5e966280ca2c130201246917\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 4: Create a Deployment for this project/model\n",
    "\"\"\"\n",
    "existing_deployments = dr.Deployment.list()\n",
    "deployment = None\n",
    "for existing_deployment in existing_deployments:\n",
    "    if (\n",
    "        existing_deployment.model and\n",
    "        existing_deployment.model['id'] == model.id and\n",
    "        existing_deployment.model['project_id'] == project.id\n",
    "    ):\n",
    "        deployment = existing_deployment\n",
    "        break\n",
    "\n",
    "if not deployment:\n",
    "    print(\"No deployment exists for this project/model yet. Creating one now.\")\n",
    "    prediction_server = dr.PredictionServer.list()[0]\n",
    "    deployment = dr.Deployment.create_from_learning_model(\n",
    "        model.id,\n",
    "        label=deployment_label,\n",
    "        description=deployment_description,\n",
    "        default_prediction_server_id=prediction_server.id\n",
    "    )\n",
    "\n",
    "print(f'Using Deployment with label \"{deployment.label}\" and ID {deployment.id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable drift tracking\n",
      "- time: 43.17\n",
      "Set association id to column \"event_id\"\n",
      "- time: 1.19\n",
      "Your Deployment is set up to use Drift Tracking and capture Association IDs! Now you can monitor it effectively.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 5: Assign an association ID to the deployment and set up drift tracking\n",
    "\"\"\"\n",
    "print('Enable drift tracking')\n",
    "t0 = time.time()\n",
    "deployment.update_drift_tracking_settings(target_drift_enabled=True, feature_drift_enabled=True)\n",
    "print('- time: %0.2f' % (time.time() - t0))\n",
    "\n",
    "t0 = time.time()\n",
    "print('Set association id to column \"%s\"' % association_id_column_name)\n",
    "deployment.update_association_id_settings(column_names=[association_id_column_name], required_in_prediction_requests=True)\n",
    "print('- time: %0.2f' % (time.time() - t0))\n",
    "\n",
    "print('Your Deployment is set up to use Drift Tracking and capture Association IDs! Now you can monitor it effectively.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## C) Submit Prediction Requests & Feed Back Actuals\n",
    "\n",
    "Now we can actually start making prediction requests against our deployment. We spoof the time of our predictions\n",
    "so that we can simulate predictions made over the course of many days.\n",
    "\n",
    "We also use this section to submit \"actuals\" – data about what the real-world outcome was for each prediction request, identifiable by our previously set Association ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 1000 predictions rows and 1000 corresponding actuals.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: Create a random sample of prediction & corresponding actual data for use in future steps.\n",
    "\n",
    "We do this simply because we might not want to send up all of our prediction data in one go,\n",
    "but instead, just a subset of it.\n",
    "\"\"\"\n",
    "\n",
    "prediction_dataset = pd.read_csv(prediction_dataset_url)\n",
    "prediction_dataset = prediction_dataset.sample(n=prediction_sample_size)\n",
    "\n",
    "actuals_dataset = pd.read_csv(actuals_dataset_url)\n",
    "\n",
    "merged_inner = pd.merge(left=actuals_dataset,right=prediction_dataset, left_on='associationId', right_on=association_id_column_name)\n",
    "actuals_dataset = merged_inner[['associationId', 'actualValue', 'wasActedOn']]\n",
    "actuals_dataset = actuals_dataset.rename(columns={'associationId': 'association_id', 'actualValue': 'actual_value', 'wasActedOn': 'was_acted_on'})\n",
    "\n",
    "print(f'Prepared {len(prediction_dataset.index)} predictions rows and {len(actuals_dataset.index)} corresponding actuals.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to trick the deployment into thinking it was created on 2019-04-15T18:26:10.014646 so that all prediction show up in the UI.\n",
      "\n",
      "<Response [202]>\n",
      "About to submit 1000 predictions across 100 chunks over the past 365 days.\n",
      "\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-04-18T18:26:10.712134\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-04-20T18:26:10.712145\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-04-22T18:26:10.712147\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-04-24T18:26:10.712149\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-04-26T18:26:10.712150\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-04-27T18:26:10.712152\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-05-01T18:26:10.712154\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-05-03T18:26:10.712155\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-05-05T18:26:10.712157\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-05-06T18:26:10.712159\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-05-07T18:26:10.712160\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-05-10T18:26:10.712162\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-05-16T18:26:10.712163\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-05-21T18:26:10.712165\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-05-24T18:26:10.712166\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-06-01T18:26:10.712168\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-06-06T18:26:10.712170\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-06-09T18:26:10.712171\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-06-11T18:26:10.712173\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-06-18T18:26:10.712175\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-06-20T18:26:10.712176\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-06-21T18:26:10.712178\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-06-25T18:26:10.712179\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-06-28T18:26:10.712181\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-07-03T18:26:10.712182\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-07-07T18:26:10.712184\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-07-10T18:26:10.712186\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-07-13T18:26:10.712187\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-07-17T18:26:10.712189\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-07-21T18:26:10.712191\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-07-24T18:26:10.712192\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-07-25T18:26:10.712194\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-07-28T18:26:10.712195\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-07-30T18:26:10.712197\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-08-06T18:26:10.712198\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-08-07T18:26:10.712200\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-08-08T18:26:10.712202\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-08-12T18:26:10.712203\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-08-13T18:26:10.712205\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-08-15T18:26:10.712207\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-08-25T18:26:10.712208\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-08-31T18:26:10.712210\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-09-02T18:26:10.712211\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-09-04T18:26:10.712213\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-09-05T18:26:10.712214\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-09-08T18:26:10.712216\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-09-10T18:26:10.712218\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-09-16T18:26:10.712219\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-09-17T18:26:10.712221\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-09-21T18:26:10.712223\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-09-23T18:26:10.712224\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-09-30T18:26:10.712226\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-10-03T18:26:10.712228\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-10-07T18:26:10.712229\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-10-09T18:26:10.712231\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-10-10T18:26:10.712232\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-10-17T18:26:10.712234\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-10-24T18:26:10.712236\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-10-27T18:26:10.712237\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-10-28T18:26:10.712239\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-10-29T18:26:10.712241\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-10-30T18:26:10.712242\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-11-03T18:26:10.712244\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-11-26T18:26:10.712245\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-11-27T18:26:10.712247\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-11-30T18:26:10.712248\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-02T18:26:10.712250\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-04T18:26:10.712251\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-05T18:26:10.712253\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-07T18:26:10.712255\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-09T18:26:10.712256\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-12T18:26:10.712258\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-14T18:26:10.712259\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-16T18:26:10.712261\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-27T18:26:10.712263\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-30T18:26:10.712265\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2019-12-31T18:26:10.712266\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-01-13T18:26:10.712268\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-01-19T18:26:10.712269\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-01-23T18:26:10.712271\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-01-26T18:26:10.712272\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-01-30T18:26:10.712274\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-02-02T18:26:10.712276\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-02-07T18:26:10.712277\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-02-13T18:26:10.712279\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-02-14T18:26:10.712280\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-02-15T18:26:10.712282\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-02-21T18:26:10.712283\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-02-23T18:26:10.712285\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-02-28T18:26:10.712287\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-03-03T18:26:10.712288\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-03-04T18:26:10.712290\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-03-06T18:26:10.712292\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-03-11T18:26:10.712293\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-03-14T18:26:10.712295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-03-15T18:26:10.712296\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-03-17T18:26:10.712298\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-03-18T18:26:10.712299\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-03-20T18:26:10.712301\n",
      "Submitted 10 predictions, spoofed to have a timestamp of 2020-03-23T18:26:10.712303\n",
      "\n",
      "Completed making 1000 predictions across 100 chunks in the past 365 days.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 2: Make some predictions, spoofed to be across a time range.\n",
    "\n",
    "We want to chunk up our predictions and spoof the time that we submitted each prediction. This allows us to\n",
    "create more realistic demo data.\n",
    "\n",
    "We do this by:\n",
    "  1) Selecting a random subset of size x from all prediction data available\n",
    "  2) Selecting n random days over the past m days\n",
    "  3) Splitting the x predictions roughly equally across the n days and submitting prediction requests for each of those days\n",
    "\"\"\"\n",
    "\n",
    "def chunk(a, n):\n",
    "    \"\"\"\n",
    "    Chunks a list up into a list of lists, with each inner list of roughly equal lengths.\n",
    "    Algorithm source: https://stackoverflow.com/a/2135920\n",
    "    \"\"\"\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "        \n",
    "def generate_random_days_in_past(number_of_days, max_days_to_go_back):\n",
    "    \"\"\"Generates a list of unique random integers, each representing a date n days before now.\"\"\"\n",
    "    random_days_in_past = random.sample(range(1, max_days_to_go_back), number_of_days)\n",
    "    random_days_in_past.sort(reverse=True)\n",
    "    random_days_in_past = [datetime.now() - timedelta(days=n_days_ago) for n_days_ago in random_days_in_past]\n",
    "    return random_days_in_past\n",
    "\n",
    "\n",
    "deployment_start_date = (datetime.now() - timedelta(days=past_n_days)).isoformat()\n",
    "print(f'About to trick the deployment into thinking it was created on {deployment_start_date} so that all prediction show up in the UI.\\n')\n",
    "spoof_deployment_start_date(deployment.id, deployment_start_date)\n",
    "\n",
    "print(f'About to submit {len(prediction_dataset.index)} predictions across {num_prediction_chunks} chunks over the past {past_n_days} days.\\n')\n",
    "\n",
    "random_days_in_past = generate_random_days_in_past(num_prediction_chunks, past_n_days)\n",
    "for index, prediction_chunk in enumerate(chunk(prediction_dataset, num_prediction_chunks)):\n",
    "    predictions_data = prediction_chunk.to_json(orient='records')\n",
    "    \n",
    "    spoofed_time_of_prediction_chunk = random_days_in_past[index]\n",
    "    make_deployment_predictions(deployment.id, predictions_data, prediction_time=spoofed_time_of_prediction_chunk)\n",
    "    print(f'Submitted {len(prediction_chunk.index)} predictions, spoofed to have a timestamp of {spoofed_time_of_prediction_chunk.isoformat()}')\n",
    "\n",
    "print(f'\\nCompleted making {len(prediction_dataset.index)} predictions across {num_prediction_chunks} chunks in the past {past_n_days} days.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed submitting 1000 actuals.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3: Feed back actuals to the deployment\n",
    "\"\"\"\n",
    "\n",
    "actuals_data = actuals_dataset.to_dict(orient='records')\n",
    "\n",
    "# Submit the actuals via the API\n",
    "deployment.submit_actuals(actuals_data)\n",
    "\n",
    "print(f'Completed submitting {len(actuals_data)} actuals.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## D) Monitor Your Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['totalPredictions', 'totalRequests', 'slowRequests', 'executionTime', 'responseTime', 'userErrorRate', 'serverErrorRate', 'numConsumers', 'cacheHitRatio', 'medianLoad', 'peakLoad']\n",
      "monitoring_window_start: 2019-04-15 00:00:00\n",
      "monitoring_window_end: 2020-04-14 00:00:00\n",
      "Deployment stats from Monday 15, 2019 to Tuesday 14, 2020\n",
      "----------------------------------------------------------\n",
      "totalPredictions: 20\n",
      "userErrorRate: 0\n",
      "cacheHitRatio: 1\n",
      "executionTime: 21\n",
      "totalRequests: 2\n",
      "serverErrorRate: 0\n",
      "slowRequests: 0\n",
      "medianLoad: 0\n",
      "numConsumers: 1\n",
      "responseTime: 209\n",
      "peakLoad: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: Get summarized Service Stats about our Deployment through a configurable window of time.\n",
    "\"\"\"\n",
    "print(SERVICE_STAT_METRIC.ALL)\n",
    "print('monitoring_window_start:', monitoring_window_start)\n",
    "# print('monitoring_window_end:', monitoring_window_end)\n",
    "# service_stats = deployment.get_service_stats(\n",
    "#     start_time=monitoring_window_start,\n",
    "#     end_time=monitoring_window_end,\n",
    "# )\n",
    "print('monitoring_window_end:', monitoring_window_end)\n",
    "service_stats = deployment.get_service_stats(\n",
    "    start_time=datetime(2019, 8, 1, hour=15),\n",
    "    end_time=datetime(2019, 8, 8, hour=15)\n",
    ")\n",
    "# service_stats = deployment.get_service_stats()\n",
    "\n",
    "print(f'Deployment stats from {monitoring_window_start.strftime(date_format)} to {monitoring_window_end.strftime(date_format)}')\n",
    "print('----------------------------------------------------------')\n",
    "for stat, value in service_stats.metrics.items():\n",
    "    print(f'{stat}: {value}')\n",
    "\n",
    "# NOTE: You may also be interested in getting these stats over time, which you can\n",
    "#       do with deployment.get_service_stats_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy stats from Monday 15, 2019 to Tuesday 14, 2020\n",
      "----------------------------------------------------------\n",
      "LogLoss:\n",
      "    Baseline: 0.610058956353653\n",
      "    Value: 0.5848363513983\n",
      "    % Change: 4.13\n",
      "AUC:\n",
      "    Baseline: 0.7026362590229103\n",
      "    Value: 0.7516499999999999\n",
      "    % Change: 6.98\n",
      "Kolmogorov-Smirnov:\n",
      "    Baseline: 0.30269902709488444\n",
      "    Value: 0.3708333333333333\n",
      "    % Change: 22.51\n",
      "Rate@Top10%:\n",
      "    Baseline: 0.8048780487804879\n",
      "    Value: 0.7722772277227723\n",
      "    % Change: -4.05\n",
      "Gini Norm:\n",
      "    Baseline: 0.40527251804582054\n",
      "    Value: 0.5032999999999999\n",
      "    % Change: 24.19\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 2: Get summarized Accuracy Stats about our Deployment through a configurable window of time.\n",
    "\"\"\"\n",
    "\n",
    "accuracy = deployment.get_accuracy(\n",
    "    start=monitoring_window_start,\n",
    "    end=monitoring_window_end\n",
    ")\n",
    "\n",
    "print(f'Accuracy stats from {monitoring_window_start.strftime(date_format)} to {monitoring_window_end.strftime(date_format)}')\n",
    "print('----------------------------------------------------------')\n",
    "for metric, values in accuracy.metrics.items():\n",
    "    print(f'{metric}:')\n",
    "    print(f'    Baseline: {values[\"baseline_value\"]}')\n",
    "    print(f'    Value: {values[\"value\"]}')\n",
    "    print(f'    % Change: {values[\"percent_change\"]}')\n",
    "          \n",
    "# NOTE: You may also be interested in getting these stats over time, which you can\n",
    "#       do with deployment.get_accuracy_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## E) Replace Your Model\n",
    "\n",
    "Here we find another model from our original Autopilot run that DataRobot recommends for a different reason,\n",
    "validate that we can replace our Deployment's current model with this new one, and then perform the replacement.\n",
    "\n",
    "Finally, we perform additional predictions against our deployment without even having to reference a model, thereby\n",
    "proving that model replacement is a seemless process thanks to the concept of Deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected a Gradient Boosted Trees Classifier model with ID 5e920f2ee5620f146d6aeb12 to use as our Deployment's new active model.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: Find an alternative model from Autopilot that DataRobot recommended for another reason.\n",
    "\"\"\"\n",
    "\n",
    "# First we try to get a model that was recommended for another reason.\n",
    "recommended_new_model = dr.ModelRecommendation.get(project.id, model_replacement_recommendation_type)\n",
    "replacement_model = dr.Model.get(recommended_model.project_id, recommended_model.model_id)\n",
    "\n",
    "# But sometimes, it is the same as the model that we started with. In this case, just\n",
    "# pick another model of a different type.\n",
    "if replacement_model.id == model.id:\n",
    "    models = project.get_models()\n",
    "    for new_model in models:\n",
    "        if new_model.id != model.id and new_model.model_type != model.model_type:\n",
    "            replacement_model = new_model\n",
    "            break\n",
    "\n",
    "print(f\"Selected a {replacement_model.model_type} model with ID {replacement_model.id} to use as our Deployment's new active model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: passing; Model can be used to replace the current model of the deployment.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 2: Validate that this model can be used to replace our Deployment's current model.\n",
    "\"\"\"\n",
    "\n",
    "status, message, checks = deployment.validate_replacement_model(new_model_id=replacement_model.id)\n",
    "\n",
    "print(f'Status: {status}; {message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Deployment is now using a Gradient Boosted Trees Classifier model with ID.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3: Actually perform the model replacement.\n",
    "\"\"\"\n",
    "\n",
    "deployment.replace_model(replacement_model.id, model_replacement_reason)\n",
    "\n",
    "print(f\"The Deployment is now using a {deployment.model['type']} model with ID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed making 10 new predictions against the deployment's new model.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 4: Submit new predictions against the replaced model.\n",
    "\n",
    "Notice that despite having a new model deployed, the call to make predictions _does not change_ from\n",
    "our previous one because we reference the deployment NOT the model. This illustrates how easy it is\n",
    "to swap models without having to change application code.\n",
    "\"\"\"\n",
    "\n",
    "new_prediction_dataset = prediction_dataset.sample(n=10)\n",
    "make_deployment_predictions(deployment.id, new_prediction_dataset.to_json(orient='records'), prediction_time=spoofed_time_of_prediction_chunk)\n",
    "\n",
    "print(f'Completed making {len(new_prediction_dataset.index)} new predictions against the deployment\\'s new model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
