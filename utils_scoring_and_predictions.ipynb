{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datarobot as dr\n",
    "import os\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data shape:       (10000, 34)\n",
      "Training data shape:   (9000, 34)\n",
      "Prediction data shape: (1000, 33)\n"
     ]
    }
   ],
   "source": [
    "# -------- Train test split the dataset --------\n",
    "\n",
    "df_full = pd.read_csv('data/DR_Demo_10K_Lending_Club_Loans.csv', encoding = 'ISO-8859-1')\n",
    "target = 'is_bad'\n",
    "\n",
    "# Shuffle the rows\n",
    "df_full = df_full.sample(frac=1, random_state=0)\n",
    "\n",
    "# Split 90% for training, 10% for predictions\n",
    "split = int(df_full.shape[0] * .1)\n",
    "df = df_full[split:]\n",
    "df_pred = df_full[:split].copy()\n",
    "\n",
    "# Drop the target from the prediction dataset\n",
    "prediction_data = df_pred.drop(target, axis=1)\n",
    "\n",
    "print('Full data shape:      ',df_full.shape)\n",
    "print('Training data shape:  ',df.shape)\n",
    "print('Prediction data shape:',prediction_data.shape)\n",
    "\n",
    "df.to_csv('data/DR_Demo_10K_Lending_Club_Loans_train.csv')\n",
    "prediction_data.to_csv('data/DR_Demo_10K_Lending_Club_Loans_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project(DR_Demo_10K_Lending_Club_Loans.csv)\n",
      "Model('Gradient Boosted Trees Classifier with Early Stopping')\n"
     ]
    }
   ],
   "source": [
    "dr.Client(token=os.environ['DATAROBOT_API_TOKEN'], endpoint='https://app.datarobot.com/api/v2')\n",
    "project_id = '5bdcda3c38f00b610eff5d8f'\n",
    "model_id = '5bdcdd9b16378b3d256a8d9c'\n",
    "\n",
    "project = dr.Project.get(project_id=project_id)\n",
    "model = dr.Model.get(project=project_id, model_id=model_id)\n",
    "datasets = project.get_datasets()\n",
    "\n",
    "print(project)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Module API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading prediction dataset\n",
      "Request predictions\n",
      "Waiting for prediction calculations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>prediction_threshold</th>\n",
       "      <th>row_id</th>\n",
       "      <th>class_0.0</th>\n",
       "      <th>class_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.234347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.765653</td>\n",
       "      <td>0.234347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.181630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818370</td>\n",
       "      <td>0.181630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.045061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.954939</td>\n",
       "      <td>0.045061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.086440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.913560</td>\n",
       "      <td>0.086440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.203154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.796846</td>\n",
       "      <td>0.203154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   positive_probability  prediction  prediction_threshold  row_id  class_0.0  \\\n",
       "0              0.234347         0.0                   0.5       0   0.765653   \n",
       "1              0.181630         0.0                   0.5       1   0.818370   \n",
       "2              0.045061         0.0                   0.5       2   0.954939   \n",
       "3              0.086440         0.0                   0.5       3   0.913560   \n",
       "4              0.203154         0.0                   0.5       4   0.796846   \n",
       "\n",
       "   class_1.0  \n",
       "0   0.234347  \n",
       "1   0.181630  \n",
       "2   0.045061  \n",
       "3   0.086440  \n",
       "4   0.203154  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------- New Scoring Predictions directly on project and model object ---------\n",
    "\n",
    "print('Uploading prediction dataset')\n",
    "dataset_from_path = project.upload_dataset('data/DR_Demo_10K_Lending_Club_Loans_pred.csv')\n",
    "\n",
    "print('Request predictions')\n",
    "predict_job = model.request_predictions(dataset_from_path.id)\n",
    "\n",
    "print('Waiting for prediction calculations')\n",
    "predictions = predict_job.get_result_when_complete()\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This installs the batch prediction help code from:\n",
    "# https://github.com/datarobot/batch-scoring\n",
    "# pip install -U datarobot_batch_scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Standard post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import pandas\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "API_TOKEN = 'xxxx'\n",
    "USERNAME = 'xxxx@example.com'\n",
    "\n",
    "DEPLOYMENT_ID = 'xxxx'\n",
    "\n",
    "# set content-type header to JSON\n",
    "headers = {'Content-Type': 'application/json; charset=UTF-8', 'datarobot-key': 'xxxx'}\n",
    "\n",
    "# create dataframe from some external source - local csv file in this case\n",
    "data = pandas.read_csv('10k_diabetes_dos.csv')\n",
    "\n",
    "# generate JSON version of the dataframe to pass to API\n",
    "data_json = data.to_json(orient='records')\n",
    "\n",
    "# deliver request to API with JSON payload\n",
    "predictions_response = requests.post('https://datarobot-support.orm.datarobot.com/predApi/v1.0/deployments/%s/predictions' % (DEPLOYMENT_ID),\n",
    "                                    auth=(USERNAME, API_TOKEN), data=data_json, headers=headers)\n",
    "\n",
    "# store the JSON response from the prediction API\n",
    "response_json = predictions_response.json()\n",
    "\n",
    "# normalize the JSON, which will flatten the data structure into a dataframe\n",
    "# details here: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\n",
    "results_df = json_normalize(data=response_json['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Batch script\n",
    "\n",
    "## https://github.com/datarobot/batch-scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U datarobot_batch_scoring\n",
    "\n",
    "batch_scoring --host=https://mycorp.orm.datarobot.com/ --user=\"greg@mycorp.com\" \n",
    "              --out=pred.csv 5545eb20b4912911244d4835 5545eb71b4912911244d4847 /home/greg/Downloads/diabetes_test.csv\n",
    "batch_scoring_sse --host=https://mycorp.orm.datarobot.com/ --out=pred.csv \n",
    "              0ec5bcea7f0f45918fa88257bfe42c09 /home/greg/Downloads/diabetes_test.csv\n",
    "batch_scoring_deployment_aware --host=https://mycorp.orm.datarobot.com/ \n",
    "              --user=\"greg@mycorp.com\" --out=pred.csv 5545eb71b4912911244d4848 /home/greg/Downloads/diabetes_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Batch script manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "PREDICTION_FILE = 'churn_dn5_date_fixed_sumgroup_recdaysle31_nodatesorextras_validationholdout.csv'\n",
    "OUTPUT_FILE = 'reason_codes_long_format.csv'\n",
    "\n",
    "PROJECT_ID = '5ab1519e5feaa758b227f436'\n",
    "MODEL_ID = '5ab3c9ba5feaa7572313d02f'\n",
    "MAX_CODES = 10\n",
    "POSITIVE_CLASS = 1\n",
    "BATCH_SIZE = 100 # how many records to send each time, typically less than 100 rows\n",
    "\n",
    "USERNAME = ''\n",
    "API_TOKEN = ''\n",
    "SERVER_URL = ''\n",
    "SERVER_KEY = ''\n",
    "\n",
    "headers = {'Content-Type': 'text/plain; charset=UTF-8', 'datarobot-key': SERVER_KEY}\n",
    "\n",
    "url_request = \"{server_url}/predApi/v1.0/{project_id}/{model_id}/reasonCodesPredictions\".format(\n",
    "        server_url=SERVER_URL, project_id=PROJECT_ID, model_id=MODEL_ID)\n",
    "\n",
    "def mini_batch_file(filepath, batch_size, encoding='utf-8'):\n",
    "    with open(filepath) as infile:\n",
    "        header = infile.readline()\n",
    "        output_lines = []\n",
    "        for line in infile:\n",
    "            output_lines.append(line.decode(encoding))\n",
    "            if len(output_lines) == batch_size:\n",
    "                output_lines.insert(0, header) # put header up front\n",
    "                yield ''.join(output_lines) # output csv text with header\n",
    "                output_lines = []\n",
    "        else:\n",
    "            output_lines.insert(0, header) # put header up front\n",
    "            yield ''.join(output_lines) # output csv text with header\n",
    "\n",
    "def get_probability_prediction(rc_json_row, positive_class):\n",
    "    return [p for p in rc_json_row['predictionValues'] if p['label'] == positive_class][0]['value']\n",
    "\n",
    "params = {'maxCodes': MAX_CODES}\n",
    "\n",
    "reason_code_lines = [] # long so will have MAX_CODES * num_record lines\n",
    "for batch_num, batch in enumerate(mini_batch_file(PREDICTION_FILE, BATCH_SIZE)):\n",
    "    try:\n",
    "        sys.stderr.write('\\r--- Making request {:,} totalling {:,} rows requested'.format(\n",
    "            batch_num+1, (batch_num+1)*BATCH_SIZE))\n",
    "\n",
    "        row_id_offset = batch_num*BATCH_SIZE # needed so row_id reflected of prediction file\n",
    "        data = batch.encode('utf-8') # encoding must match in headers\n",
    "        predictions_response = requests.post(url_request,\n",
    "                                            auth=(USERNAME, API_TOKEN),\n",
    "                                            data=data,\n",
    "                                            headers=headers,\n",
    "                                            params=params,\n",
    "                                            timeout=120)\n",
    "        if predictions_response.status_code != 200:\n",
    "            try:\n",
    "                message = predictions_response.json().get('message', predictions_response.text)\n",
    "                status_code = predictions_response.status_code\n",
    "                reason = predictions_response.reason\n",
    "\n",
    "                print(u'Status: {status_code} {reason}. Message: {message}.'.format(message=message,\n",
    "                                                                                    status_code=status_code,\n",
    "                                                                                    reason=reason))\n",
    "            except ValueError:\n",
    "                print('Prediction failed: {}'.format(predictions_response.reason))\n",
    "                predictions_response.raise_for_status()\n",
    "        else:\n",
    "            reason_code_prediction_rows = predictions_response.json()['data']\n",
    "            for rc_json_row in reason_code_prediction_rows:\n",
    "                prediction = get_probability_prediction(rc_json_row, POSITIVE_CLASS)\n",
    "                row_id = rc_json_row['rowId'] + row_id_offset\n",
    "                for rc in rc_json_row['reasonCodes']:\n",
    "                    rc['prediction'] = prediction\n",
    "                    rc['row_id'] = row_id\n",
    "                    reason_code_lines.append(rc)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "pd.DataFrame(reason_code_lines).to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deployment predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://app.datarobot.com/api/v2'\n",
    "headers2 = {'Content-Type': 'application/json', 'Authorization': 'token %s' % API_TOKEN}\n",
    "\n",
    "health_response = requests.get('https://app.datarobot.com/api/v2/modelDeployments/%s/' % (DEPLOYMENT_ID), headers=headers2)\n",
    "\n",
    "dr = datarobot.Client(endpoint=ENDPOINT, token=API_TOKEN)\n",
    "\n",
    "if health_response.json()['modelHealth'] == 'failing':\n",
    "    model = datarobot.Model.get(model_id=health_response.json()['model']['id'], project=health_response.json()['project']['id'])\n",
    "    retrainProject = datarobot.Project.start(sourcedata='/Users/james.johnston/Python/10K_Lending_Club_Loans.csv', project_name='Lending Club Retrain', target='is_bad', \\\n",
    "        autopilot_on=False)\n",
    "    modelJobId = retrainProject.train(model.blueprint)\n",
    "    newModel = datarobot.models.modeljob.wait_for_async_model_creation(project_id=retrainProject.id, model_job_id=modelJobId)\n",
    "    fi = newModel.get_or_request_feature_impact(600)\n",
    "    model_Update = requests.patch('https://app.datarobot.com/api/v2/modelDeployments/%s/model' % (DEPLOYMENT_ID), headers=headers2, data=\"{'modelId':'%s'}\" % newModel.id)\n",
    "    pprint.pprint(model_Update)\n",
    "\n",
    "\n",
    "pprint.pprint(health_response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch stuff...\n",
    "\n",
    "# \n",
    "# # Upload the scoring dataset if not already uploaded, else retrieve it\n",
    "# if (len(datasets) == 0):\n",
    "#     print('Uploading prediction dataset')\n",
    "#     pred_ds = project.upload_dataset(prediction_data)\n",
    "# else:\n",
    "#     print('Retrieving the prediction dataset id')\n",
    "#     for s in datasets:\n",
    "#         if s.name == 'predict.csv':\n",
    "#             pred_ds = s\n",
    "\n",
    "# # Request predictions for the scoring dataset, if it hasn't been already requested\n",
    "# try:                      \n",
    "#     predict_job\n",
    "# except NameError:\n",
    "#     print('Requesitng predictions')\n",
    "#     predict_job = model.request_predictions(pred_ds.id)\n",
    "    \n",
    "# # Get the predictions results when complete\n",
    "# print('Waiting for prediction results')\n",
    "# predictions = predict_job.get_result_when_complete()\n",
    "# print('- done.  Predictions dataframe:')\n",
    "# print(predictions.head())\n",
    "\n",
    "# # Add a column for the target labels (y_true)\n",
    "# predictions['y_true'] = df_pred[target].astype(int).tolist()\n",
    "# print('\\n',predictions.head())\n",
    "\n",
    "# y_true = predictions['y_true'].values\n",
    "# y_pred = predictions['class_1.0'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 igor's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- Training Predictions ---------\n",
    "\n",
    "try:\n",
    "    # Calculate new training predictions on holdout partition of dataset\n",
    "    training_predictions_job = model.request_training_predictions(dr.enums.DATA_SUBSET.ALL)\n",
    "    print('Request training predictions, waiting for results')\n",
    "    training_predictions = training_predictions_job.get_result_when_complete()\n",
    "\n",
    "    # Fetch training predictions as data frame\n",
    "    df_dr_train_preds = training_predictions.get_all_as_dataframe()\n",
    "    print('- Done')\n",
    "#     df_dr_train_preds.head()\n",
    "except Exception as e:\n",
    "    print('Already requested training predictions')\n",
    "    # Fetch all training predictions for a project\n",
    "    all_training_predictions = dr.TrainingPredictions.list(project_id)\n",
    "\n",
    "    # Inspect all calculated training predictions\n",
    "    for training_predictions in all_training_predictions:\n",
    "        print(\n",
    "            'Prediction {} is made for data subset \"{}\" of model {}'.format(\n",
    "                training_predictions.prediction_id,\n",
    "                training_predictions.data_subset,\n",
    "                training_predictions.model_id\n",
    "            )\n",
    "        )\n",
    "        if training_predictions.model_id == model_id:\n",
    "            print('Getting training preds for model', training_predictions.model_id)\n",
    "            prediction_id = training_predictions.prediction_id\n",
    "#             df_dr_train_preds = training_predictions.get(project_id, prediction_id)\n",
    "            df_dr_train_preds = training_predictions.get_all_as_dataframe()\n",
    "            print('- Done')\n",
    "#             print(df_dr_train_preds.head())\n",
    "\n",
    "df_dr_train_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you changed the threshold, subset at about that point to see that infact we are labeling by it...\n",
    "df_dr_train_preds[df_dr_train_preds['class_1.0'] > .28].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
